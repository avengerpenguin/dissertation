\documentclass{sig-alternate-05-2015}

\usepackage[autosize]{dot2texi}
\usepackage[dvipsnames]{xcolor}
\usepackage{textcomp}
\usepackage{caption}
\usepackage{listings}
\usepackage{tikz}
\usepackage{rotating}

% Language Definitions for SPARQL
\lstdefinelanguage{sparql}{
morestring=[b][\color{blue}]\",
morekeywords={SELECT,CONSTRUCT,DESCRIBE,ASK,WHERE,FROM,NAMED,PREFIX,BASE,OPTIONAL,FILTER,GRAPH,LIMIT,OFFSET,SERVICE,UNION,EXISTS,NOT,BINDINGS,MINUS,a},
sensitive=true
}

\colorlet{punct}{red!60!black}
\definecolor{background}{HTML}{EEEEEE}
\definecolor{delim}{RGB}{20,105,176}
\colorlet{numb}{magenta!60!black}

\lstdefinelanguage{json}{
    basicstyle=\normalfont\ttfamily\footnotesize,
    numbersep=8pt,
    showstringspaces=false,
    breaklines=true,
    literate=
     *{0}{{{\color{numb}0}}}{1}
      {1}{{{\color{numb}1}}}{1}
      {2}{{{\color{numb}2}}}{1}
      {3}{{{\color{numb}3}}}{1}
      {4}{{{\color{numb}4}}}{1}
      {5}{{{\color{numb}5}}}{1}
      {6}{{{\color{numb}6}}}{1}
      {7}{{{\color{numb}7}}}{1}
      {8}{{{\color{numb}8}}}{1}
      {9}{{{\color{numb}9}}}{1}
      {:}{{{\color{punct}{:}}}}{1}
      {,}{{{\color{punct}{,}}}}{1}
      {\{}{{{\color{delim}{\{}}}}{1}
      {\}}{{{\color{delim}{\}}}}}{1}
      {[}{{{\color{delim}{[}}}}{1}
      {]}{{{\color{delim}{]}}}}{1},
}

\lstset{frame=single,captionpos=b}

\begin{document}

\title{Improving content discovery through combining linked data and data mining techniques}
%\subtitle{[Extended Abstract]

\numberofauthors{1}
\author{
\alignauthor
Ross Fenning\\
       \affaddr{British Broadcasting Corporation (BBC)}\\
       \affaddr{MediaCityUK}\\
       \affaddr{Salford, United Kingdom}\\
       \email{ross.fenning@bbc.co.uk}
}

\maketitle
\begin{abstract}
A comparative study of multiple approaches for extracting linked data
from web content published by the BBC for the purposes of applying
machine learning to cluster related content.

This project designs and implements a process by which semantic data
about any piece of online media content can be extracted into RDF
models in
multiple ways and feature
sets for machine learning can be generated from
that data. The clusters produced by fourteen different permutations
of techniques are evaluated and a potential application of suggesting
related content is mocked up for qualitative evaluation.

It is concluded organisations can very easily make use of
embedded semantics within web pages due to better semantic properties
being made available in recent years because of pressure from
social media sites such as Facebook and Twitter.
These embedded semantics provide a good baseline for simple
learning that can group content based on broad categorisation.

A simple application of entity extraction also shows promising results in
evaluation and is highlighted as an area for organisations to explore
tuning to provide a powerful complement to embedded semantics.
\end{abstract}

\section{Introduction}

Media companies produce ever larger numbers of articles, videos, podcasts,
games, etc. -- commonly collectively known as ``content''. A successful
content-producing website not only has to develop systems to aid producing and
publishing that content, but there are also demands to engineer effective
mechanisms to aid consumers in finding that content. Examples include providing a
text-based search, hierarchical
categorisation and tailored recommended
content based on past behaviour.

\subsection{Problems}

\begin{itemize}

\item Content across multiple content management
systems, in differing formats and data models.

\item Content items are in fairly opaque formats, e.g. video and audio content.

\item Content is being published continuously, so any analytical process that operates
over all content (e.g. machine learning) may need to be run periodically or in
an incremental fashion.

\end{itemize}

\subsection{Hypothesis}

\begin{itemize}

\item Research and software tools around the concept of \emph{Linked Data} can
aid us in rapidly acquiring a broad view of an
organisation's content.

\item We can establish at least a na\"ive mapping of an RDF graph representing a
content item to an attribute set suitable for data mining.

\item Linked Data and Semantic Web \emph{ontologies} and models available can
provide data enrichment beyond attributes and keywords explicitly avaiable
within content data or metadata.

\item We can make use of semantic metadata added to pages
to provide better presentation of that content as it is shared on social media.

\end{itemize}

\section{Background}

Data mining activities such as machine learning rely on structuring data as
\emph{feature sets}\cite{bishop2006pattern} -- a set or vector of properties or
attributes that describe a single entity.
The process of \emph{feature extraction}
generates such feature sets from raw data and is a necessary early phase for
many machine learning activities.

The RDF graph is a powerful model
for metadata based on representing knowledge as a set of
subject-predicate-object \emph{triples}. The query language, SPARQL, gives us a
way to query the RDF graph structure using a declarative pattern and return a
set of all variable bindings that satisfy that pattern.

For a large content-producer, there is a more general problem where many content
items do not have a relational representation and the content source is a body
of text or even a raw HTML page. However, the feature generation from Paulheim
and F\"urnkranz proves to be a promising strategy given we can acquire an RDF
graph model for content items in the first place.

\section{Design}

An experiemental system was designed and built around the following
high-level requirements:

\begin{enumerate}
\item gathering (meta)data about all of an organisations content items;
\item further enriching that metadata with information not explicitly present
in the content item itself; and
\item applying machine learning to that content metadata to gain new insights
about that content.
\end{enumerate}

The system was also designed with the end goal of being able to present
users with alternative content items deemed similar to any given item they
are currently ready or watching. This provided a focus to ensure some
industrial application of the experiment and also a means by which to evaluate
the results in that results can be scored on how they perform at driving
such a website feature.

\begin{figure}[h]
  \begin{center}
    \includegraphics[width=\linewidth]{../report/diagrams/component.png}
  \end{center}
  \caption{High-level component diagram with interfaces for each use case\label{fig:component}}
\end{figure}

Figure~\ref{fig:component} shows a high-level view of a ``Content Miner''
software application. A set of ``notifier''
applications can be created to connect different content production and
management systems to the data mining system such that it is notified when new
content is created. The notification need only be an \emph{IRI} that uniquely
identifies that content item. Once the application has meaningful clusters of
content, we can imagine another system that can query this miner application
to provide some view of related content given some initial content item.

\subsection{Data Pipeline}
\label{sec:design-pipeline}

This input side of the miner was conceptually modelled as a data ``pipeline'' that takes
an IRI identifying a piece of web content at the source end and produces
feature sets at the other end. In this pipeline, the three primary data
structures employed are the \emph{IRI}\footnote{http://tools.ietf.org/html/rfc3987}
(in many cases the public URL for the content's primary page is sufficient),
the \emph{feature set} (modelled as a schema-less set of key-value pairs)
and the \emph{RDF graph} as an intermediate structure for manipulating along
the pipeline.

The data pipeline was then designed inductively, introducing potential
methods for creating RDF graphs from web content and also enriching those
graphs.

Listings~\ref{lst:simple-sparql} shows a SPARQL query inspired by Paulheim and F\"urnkranz\cite{paulheim2012unsupervised}.
This
query generates a boolean \texttt{true} value for any properties that match
and implies \texttt{false} for those that do not and we then flatten this into
a simple feature format: \texttt{?p\_?o=true},
e.g. \texttt{rdf:type\_schema:Article=true}

\begin{lstlisting}[label=lst:simple-sparql,caption={Generates field \texttt{content\_?p\_?v} with value \texttt{true}},language=sparql]
SELECT ?p ?o
WHERE { ?iri ?p ?o . }
FILTER isIRI(?o)
\end{lstlisting}

Note that this ignores triples whose objects are literals, for which case the
similar strategy as shown in listings~\ref{lst:literal-sparql} was employed to
produce features of the form: \texttt{?p=?v}.

\begin{lstlisting}[label=lst:literal-sparql,caption={Generates field \texttt{content\_?p\_?v} with value \texttt{true}},language=sparql]
SELECT ?p ?v
WHERE { ?iri ?p ?v . }
FILTER isLiteral(?o)
\end{lstlisting}

With a feature generation strategy in place, we can inductively build up the
pipeline of how we create RDF graphs for that generation.

\begin{centering}
\begin{lstlisting}[label=lst:jsonld-identity,caption={Identity graph for a content item in JSON-LD syntax},language=json]
{
  "@id": "http://example.com/entity/1",
  "@graph": []
}
\end{lstlisting}
\end{centering}

With the knowledge only of a content item's IRI, we are arguably only able to
produce an empty named RDF graph. Such a graph for an example IRI
\texttt{http://example.com/entity/} is illustrated in
JSON-LD syntax in Listings~\ref{lst:jsonld-identity}. This provides our
base case in the inductive design. This enables a \emph{null} pipeline
as shown in Figure~\ref{fig:gen-null} that only produce feature sets with
a single property containing the item's IRI.

            \begin{figure}[h]
              \begin{center}
                \begin{dot2tex}[dot,options=-t math,autosize,pgf,scale=0.7]
                  digraph g {
                    rankdir=LR;

                    node [shape=circle,margin="0,0"];
                    edge [len=2];

                    IRI -> RDF [label="extract"];
                    RDF -> Features [label="(IRI)"];
                  }
                \end{dot2tex}
              \end{center}
              \caption{Null feature generator \label{fig:gen-null}}
            \end{figure}

We can then add in, one-by-one, the following potential extraction techniques:

\begin{itemize}
\item Extraction by dereference of the IRI, parsing any RDFa or microdata
semantics found embedded in the response.
\item Use entity extraction such as DBpedia and infer triples of the form
$(IRI, rdf\!\!:\!\!about, Entity)$
\item Infer a relationship between pages if they have hyperlinks to each
other: $(IRI_1, ex:related, IRI_2)$
\end{itemize}

\begin{figure*}
  \begin{center}
    \begin{dot2tex}[dot,options=-t math,autosize,pgf,scale=0.6]
      digraph g {
        rankdir=LR;

        node [shape=circle,margin="0,0"]
        RDF1 [label="RDF_1"];
        RDF2 [label="RDF_2"];
        RDF3 [label="RDF_3"];
        RDFp [label="RDF'"];
        RDFp1 [label="RDF'_1"];
        RDFp2 [label="RDF'_2"];
        RDFp3 [label="RDF'_3"];
        RDFp4 [label="RDF'_4"];

        IRI -> RDF1 [label="\text{parse}"];
        IRI -> RDF2 [label="\text{extract}"];
        IRI -> RDF3 [label="\text{scrape}"];

        RDF1 -> RDF [label="\cup"];
        RDF2 -> RDF [label="\cup"];
        RDF3 -> RDF [label="\cup"];

        RDF -> RDFp1 [label="\text{dereference/parse}"];
        RDF -> RDFp2 [label="\text{hypernym inference}"];
        RDF -> RDFp3 [label="\text{expert inference}"];
        RDF -> RDFp4 [label="\text{OWL inference}"];

        RDF -> RDFp [label="\cup"];
        RDFp1 -> RDFp [label="\cup"];
        RDFp2 -> RDFp [label="\cup"];
        RDFp3 -> RDFp [label="\cup"];
        RDFp4 -> RDFp [label="\cup"];

        RDFp -> Features [label="\text{content\_?p\_?v}"];
        RDFp -> Features [label="\text{content\_?p1\_?p2\_?v}"];
      }
    \end{dot2tex}
  \end{center}  \caption{\label{fig:maximal-pipeline}Maximal Data Pipeline}
\end{figure*}

We also propose enrichment techniques such as the following:

\begin{itemize}
\item Dereference every object IRI for triples of which the given content item
is the subject.
\item Infer facts based on OWL/RDFS ontologies and schemas, e.g. infer an
item is about animals if the entity \emph{Dog} is found and we have an
ontology that states that \emph{Animal} is a superclass.
\end{itemize}

This could lead to a theoretical maximum pipeline as depicted in Figure~\ref{fig:maximal-pipeline}.

\section{Implementation}

All functionality was implemented as part of a single Python module
named \emph{distillery}
that is run via the Unix command line, with different subcommands for
each feature. Each command was designed around the
Unix Philosophy\cite{raymond2003art} of doing one job per command and
that they were composable via Unix pipes. This allows pipelines to
be composed, e.g.:

\begin{centering}
  \begin{lstlisting}[
      basicstyle=\scriptsize,
      label=lst:unix-pipe,
      language=bash]
distillery extract <iris.txt | distillery enrich
  \end{lstlisting}
\end{centering}

\noindent that resemble the entire theoretical data pipeline described
in section~\ref{sec:design-pipeline} without the need for middleware
such as message queues. More typical enterprise architectures around
message oriented middleware or event-driven architecture might need
to be employed to scale this system to millions of documents, but
the simple approach above is sufficient for tens of thousands of
documents.

\section{Analysis and Evalaution}

Clusters produced were analysed both by an exploratory analysis of the results
and also a survey given to 30 respondents to evaluate mock-ups of
suggested, "related" content based on a hypothetical starting page and
promoted links to other pages in the same cluster.

Exploratory analysis showed a large variety in the sizes of clusters produced.
Hyperlink relationships fell down on finding links to common pages like FAQs
which then allowed the greedy clustering algorithms to spend too much time
grouping all pages that linked to those very common links.

Embedded semantics did well where annotations were put in place for social
media sharing. There was some difficulties with noise from semantic vocabularies
concerned with describing structure of the page or even schema properties
about the properties themselves.

Entity extraction was strong at finding larger numbers of triples, but required
some monitoring to avoid false positives from words common to many pages
(a large number of BBC pages mention Twitter, for example, where there
are controls prompting users to share content on that platform). Other examples
included a ``Listen'' button present on all BBC pages about radio programmes.

Enrichment by dereference had the same shortcomings of embedded semantics
extraction and whilst it did add value in some cases, it generated an explosive
increase in the number of triples that had a large impact on system performance.

Results from the survey supported the idea that embedded entities pleased
users where it was able to group things on very structural, categorical aspects
of the data (e.g. BBC News vs. BBC iPlayer or even ``Football'' category
within BBC Sport.)

However, many users preferred items to be linked more strongly by theme or
topic. Some useful discussion with user experience experts supported this idea
and in some cases this was achieved due to entity extraction.

\section{Conclusions}

Extracting embedded semantics provided enough initial benefits for machine
learning in that a model can be produced that understands the broad categorisation
of media content. That it is far simpler to implement than bespoke enterprise
integration projects, proves its worth as an initial iteration of a more
intelligent system.

Organisations would benefit from implementing this baseline for gaining
insights into its content and then spend later iterations looking at
introducing entity extraction, monitored carefully for appropriate tuning.

Later iterations can look at some enterprise integration work to improve
the quality of the most poorly-performing content items. there is certainly
need for future work to evaluate some of the more complex enrichment approaches.

\bibliographystyle{abbrv}
\bibliography{../report/references}

\end{document}
