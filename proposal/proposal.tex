\documentclass[10pt,a4paper]{article}

\usepackage{listings}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\usepackage[autosize]{dot2texi}
\usepackage{xcolor}

\usepackage{fullpage}

\title{Proposal for improving content discovery through combining linked data
and data mining techniques}

\author{Ross Fenning}

\date{}

\begin{document}

\maketitle \thispagestyle{empty}

\section{Context}

Media companies produce ever larger numbers of articles, videos, podcasts,
games, etc. -- commonly collectively known as ``content''. A successful
content-producing website not only has to develop systems to aid producing and
publishing that content, but there are also demands to engineer effective
mechanisms to aid consumers in finding that content.

Approaches used in industry include providing a text-based search, hierarchical
categorisation (and thus navigation thereof) and even more tailored recommended
content based on past behaviour or content enjoyed by friends (or sometimes
simply other consumers who share your preferences).

\section{Problems}

There are several technical and conceptual problems with building effective
content discovery mechanisms, including:

\begin{itemize}

\item Large organisations can have content across multiple content management
systems, in differing formats and data models. Organisations face a large-scale
enterprise integration problem simply trying to gain a holistic view of all
their content.

\item Many content items are in fairly opaque formats, e.g. video content may be
stored as audio-visual binary data with minimal metadata to display on a
containing web page. Video content producers may not be motivated to provide
data attributes that might ultimately be most useful in determining if a user
will enjoy the video.

\item Content is being published continuously, which means any search or
discovery system needs to keep up with content as it is published and process it
into the appropriate data structures. Any machine learning previously performed
on the data set may need to be re-run.

\end{itemize}

\section{Hypotheses}

In this proposal, the following hypotheses are proposed for research:

\begin{itemize}

\item Research and software tools around the concept of \emph{Linked Data} can aid
us in rapidly acquiring a broad view (perhaps at the expense of depth) of an
organisation's content whilst also providing a platform for simple enrichment of
that content's metadata.

\item We can establish at least a na\"ive mapping of an RDF graph representing a
content item to an attribute set suitable for data mining. With such a mapping,
we can explore applying machine learning -- particularly unsupervised learning
-- across an organisation's whole content corpus.

\item Linked Data and Semantic Web \emph{ontologies} and models available can
provide data enrichment beyond attributes and keywords explicitly avaiable
within content data or metadata.

\item We can adapt established machine learning approaches such as clustering
for data published continuously in real time.

\item Many content-producers currently enrich their web pages with small
amounts of semantic metadata to provide better presentation of that content
as it is shared on social media. This enables simple collection of a full
breadth of content with significantly less effort than direct integration
with content management systems.

\end{itemize}

\section{Proposal}

It is thus proposed to do the following research activities:

\begin{enumerate}

\item Create a simple software system that will fetch a content item by its
URI, extract semantic data readily available in the page (RDFa, microformats,
social media metadata) and store an RDF graph.

\item Define a simple mapping from these RDF graphs and experiment with
unsupervised learning such as clustering. It should be possible to create a
basic web page that visualises clusters of content as they emerge.

\item Experiment further with the effectiveness of each of the following:
\begin{itemize}
\item Using established entity extraction on textual content to generate new
attributes for a content item's RDF graph.
\item Using linguistic knowledge from taxonomies (e.g. hypernyms) to infer
further attributes that are implicit (i.e. intuitive to humans, but not
explicitly present in the content).
\item Using established ontologies and linked data to enrich a content item
with transitive attributes from related entities (e.g. the birth place of an
actor that appeared on a television programme).
\item Identifying opportunities to apply domain knowledge from experts.
\end {itemize}

\end{enumerate}

A desired outcome is an ecosystem of software services that can receive
notifications of newly-published content, produce highly-enriched RDF graphs,
perform real-time data mining and drive a single web page that allows navigation
of any machine learning insights.

That there is little in the literature around combining linked data and machine
learning indicates that this research could point to the need for deeper
exploration of one or more of the research activities above. It is hoped that
this work will at least provide a good indication of which approaches appear to
give the best opportunties for content metadata enrichment and insights.

It is also hoped that this work provides a good balance of opening up further
research opportunities around data mining of linked data graphs along with some
practical tools and techniques that media organisations could apply immediately
to get a little more effective at providing navigation and discovery of their
content.

\end{document}

\begin{comment}

This proposal essentially brings together the fields of linked data/semantic web
and data mining, with some possibilities of stepping into natural language
processing. The premise is to handle finding and curating amongst large
quantities of online web content produced by media entities like the BBC. We can
use linked data and the semantic web to acquire representations of those content
items at scale and enrich what we know about them by following links, unifying
with other data sources and by performing logical inference (perhaps even using
linguistic semantics to aid what we can infer).

Once we have adequately acquired data on content, we can look at machine
learning techniques and visualising consequent models to users as a discovery
portal of sorts. My first instinct is to look at clustering content items so as
to provide onward journeys to similar items that might not otherwise have been
seen by users (and perhaps conversely make use of dissimilarity to provide more
tangential journeys). This is meant as a divergence from -- but a useful adjunct
to -- information retrieval approaches where similarity focuses on keywords
extracted from text. I may find time to explore the benefits of other machine
learning techniques.

I'm seeing some novelty in 1) handling unsupervised learning and enrichment of
data as it arrives in real time (content is consistently published as opposed to
working on static data sets); and 2) the benefits of the linked data research
and tools that already exist that can provide methods for rapid acquisition and
transformation of data ahead of machine learning. There are also possibilities
to touch on natural language research and entity extraction to improve the
enrichment side of the system to improve the machine learning side further.

\end{comment}
