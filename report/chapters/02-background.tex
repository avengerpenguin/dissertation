\chapter{Background}

This chapter discusses some of the existing research and technologies
around machine learning, RDF and combining them. It also covers some
of the advantages of using linked data and RDF in an enterprise
setting and what tools and approaches are well-defined enough that a
corporation could build on top of them rapidly.

Data mining activities such as machine learning rely on structuring
data as \emph{feature sets}\cite{bishop2006pattern} -- a set or vector
of properties or attributes that describe a single entity.
The process of \emph{feature extraction} generates such feature sets
from raw data and is a necessary early phase for many machine learning
activities.

The rest of this chapter will show:

\begin{enumerate}
\item that extracting feature sets from
  RDF\footnote{http://www.w3.org/TR/PR-rdf-syntax/} graphs can be done
  elegantly and follows naturally from some previous work in this area; and
\item that the RDF graph is a suitable and even desirable data model for content
  metadata in terms of acquiring, enriching and even transforming that data ahead
  of feature extraction.
\end{enumerate}

\section{Data Mining}
\subsection{Overview}

\emph{Data Mining} is a process for finding knowledge and information
in data sets by drawing on research from artificial intelligence,
machine learning and statistics.\cite{han2011data} In this research,
the focus is on the machine learning aspect with
the data extraction phase implemented using semantics and RDF.

\subsection{Supervised Learning}

\emph{Machine Learning} broadly breaks down into \emph{supervised}
and \emph{unsupervised} learning. In supervised learning, we have
a task with a clear goal for which we attempt to create a computer
model that can perform that task. In this context, a task might be
to categorise an entity based on its attributed (e.g. diagnose
a particular illness based on symptoms and assessment data from a
patient exam) or perhaps a more numerical goal such as predicting
the chance of rain on a given day based on multiple numerical
readings.

Well-known examples include neural networks, decision trees and or
linear regression. In all supervised learning techniques, we can
normally assess the model's performance against training data for
which we already know the answers. We can see supervised learning
thus as a computational attempt to extrapolate human knowledge into
a reusable model that can predict or determine things at a speed
and scale humans would not normally be able to work through manually
(e.g. predicting weather with manual calculations or diagnosing
patients more quickly and with greater accuracy).

\subsection{Unsupervised Learning}

In unsupervised learning, there is not necessarily a specific goal
against which we can judge the models, but instead we are applying
more exploratory methods to tell us patterns in the data that maybe
we could not see for ourselves (e.g. if we would have to graph trends
over more than two or three dimensions, it is usually difficult to
visualise).

A well-known example is association rule mining that can find
interesting relations in data sets that were perhaps previously
unknown. A classic implementation of this technique is in
supermarkets and other commercial enterprises where those
organisations seek to find rules and patterns in consumer purchasing
habits so that they can try to promote items that customers might
want to buy based on other items they are currently trying to
purchase.

\subsection{Clustering}

Another major unsupervised learning technique -- and the focus of
this research -- is \emph{clustering}. In clustering algorithm, we
seek to group similar items or entities together in groups or
\emph{clusters}. Clustering has many applications, but in this
work the focus will be around \emph{hierarchical} clustering which
is effective for grouping together items that appear to be
connected in some way -- in our case that the media content is
connected by virtue of a common theme, topic or genre.

This is based on the premise that users that like a particular
content item might well be interested in other items that mention
the same topics, fall into the same genres or are in some other
way linked.

Note that association rule mining is arguably useful in a similar way.
In the case where we know from user behaviour that users like e.g.
content item $A$ also like content item $B$, then a new user that
likes $A$ but has not read or watched $B$ is likely to be interested
in $B$, so we can suggest it to them.

This is very powerful -- perhaps more powerful as it models how
people behave, not arbitrary tagging or classification of content --
but the focus of this research experiment is to explore the insights
we can get \emph{from the content data alone}. This is appealing
where content producers are struggling to understand their users'
behaviour as collecting meaningful analytics and records of user
behaviour is by no means a trivial endeavour in its own right.

Hierarchical clustering differs from, say,
\emph{k-means clustering}\cite{witten2005data} in that
we start with no assumption about the number of clusters or groups
the data will be divided into. This method is generally applied
\emph{agglomeratively} or \emph{divisively}.

In divisive, hierarchical clustering, we start with all elements in
a single cluster and proceed to find divisions that best split that
cluster. This is applied recursively until we generate a
dendrogram\cite{witten2005data} showing a full hierarchy of similarity
between the items or we can halt the division based on some condition,
e.g. only split clusters with a low cohesion below some threshold
and stop on the first attempt to split a cohesive cluster above
that threshold.

Agglomerative, hierarchical clustering goes in the opposite direction
by starting with all items in their own, individual clusters.
The two ``nearest'' clusters are then recursively merged until, again,
we have a full dendrogram or stop before we are about to create a
cluster whose cohesion falls below some threshold.

\subsection{Distance and Linkage}

Clustering relies on grouping items that seem close, which creates
a necessity to define what is meant by close. The distance between
any two items is normally determined by a distance
function.\cite{witten2005data} Such a function will normally take
two items and return a value between 0 (items are the same) up to an
an unbounded value as the items are considering further and further
apart.

Related to distance and specific to clustering is the concept of
linkage, which describes the distance between two clusters. A linkage
function will make use of a distance function. Three examples
of linkage functions include:

\begin{itemize}
\item \emph{Complete linkage} between two clusters is defined as the
  greatest distance of all possible pairs of items, one from each
  cluster.
\item \emph{Single linkage} is similar to complete, but the smallest
  distance is used.
\item \emph{Mean linkage} uses the mean where minimum or maximum
  are used above.
\end{itemize}

\subsection{Clustering Media Content}
\label{sec:clustering-media}

For media content, agglomerative clustering is a good match for two
practical reasons:

\begin{enumerate}
\item Content is continuously published, so there is potential
  to consider a ``real-time'' clustering system where new items
  arrive in their own cluster initially and are merged into an
  appropriate, established cluster when the next merge is performed.
  This is out of scope for this work, but is an interesting
  consideration if an organisation is to apply this to its real
  content in a way that keeps up to date with new content as it is
  published.
\item The BBC has of the order of tens of millions of pages and thus
  the content is likely to be very diverse with little to no overlap
  between the vast majority of it. The agglomerative model appears to
  fit better with the idea of merging together items where small
  islands of similar content are found and simply ignoring the large
  ``dark matter'' of pages that don't easily merge or for which we
  cannot easily extract good metadata (e.g. if the pages are very
  old).
\end{enumerate}

In this experiment, Jaccard distance\cite{witten2005data}
based on its suitability for binary, asymmetric data. It is argued
that the vast vocabulary offered by all potential semantic web
RDF triples will lead to very asymmetric binary values based on the
presence or non-presence of that triple.

The chosen linkage function was the complete linkage
on the basis that it ensures that any two content items
within a cluster to be considered related. This avoids the so-called
\emph{chaining phenomenon} where single linkage may create clusters
where two unrelated items are members because they are each related
to an item of chain of related items
therebetween.\cite{everitt2011hierarchical}

\section{RDF and Feature Extraction}
\label{sec:rdf-and-features}

The RDF graph is a powerful model
for metadata based on representing knowledge as a set of
subject-predicate-object \emph{triples}. The query language, SPARQL, gives us a
way to query the RDF graph structure using a declarative pattern and return a
set of all variable bindings that satisfy that pattern.

For example, the SPARQL query in Listings~\ref{lst:sparqlfoaf}
queries an RDF graph that contains contact information and returns the
names and email address of all ``Person'' entities therein.

Notably, Kiefer, Bernstein and Locher\cite{kiefer2008adding} proposed a novel
approach called SPARQL-ML -- an extension to the
SPARQL\cite{segaran2009programming} query language with new keywords to
facilitate both generating and applying models. This means that the system
capable of parsing and running standard queries must also run machine learning
algorithms.

Their work involved developing an extension to the SPARQL query
engine for \emph{Apache Jena}\footnote{https://jena.apache.org/} that integrates
with systems such as \emph{Weka}\footnote{http://www.cs.waikato.ac.nz/ml/weka/}.
A more suitable software application for enterprise use might focus solely on
converting RDF graphs into a neutral data structure that can plug into arbitrary
data mining algorithms.

\begin{lstlisting}[label=lst:sparqlfoaf,caption={Example SPARQL query for people's names and email addresses},language=sparql]
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name ?email
WHERE {
  ?person a foaf:Person.
  ?person foaf:name ?name.
  ?person foaf:mbox ?email.
}
\end{lstlisting}

If we consider an RDF graph, $g$, to be expressed as a set of triples:

\begin{displaymath}
  (s, p, o) \in g
\end{displaymath}

\noindent this query could then be
expressed as function $f: G \rightarrow (S \times S)$ where $G$ is the set of
all possible RDF graphs and $S$ is a set of all possible strings.
This allows the result of the
SPARQL query to be expressed as a set of all SELECT variable bindings that
satisfy the WHERE clause:

$$
q(g,n,e) = \exists p . (p, type, Person) \in g\ \land (p, name, n) \in g \land (p, mbox, e) \in g
$$

$$
g \in G \models f(g) = \{(n, e) \subseteq (S \times S) \: | \: q(g,n,e)\}
$$

This could be generalised to express a given feature set as
vector $(a_1, a_2, ..., a_n)$:

$$
g \in G \models (a_1, a_2, ..., a_n) \in f(g)
$$

\noindent and in the case where all $a_k \in f(g)$ are literal (e.g. string or
numeric) values, we can thus consider a given SPARQL query to be specific
function capable of feature extraction from any RDF graph into sets of
categorical or numeric features.

\begin{lstlisting}[label=lst:sparqlabout,caption={SPARQL query to determine what },language=sparql]
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
SELECT ?topic
WHERE {
  ?article rdf:about ?topic .
}
\end{lstlisting}

This might allow a query that extracts a country's population, GDP, etc.
provide feature extraction for learning patterns in economics, for example.
However, this is limited to features derived from single-valued predicates
with literal-valued ranges. It is not clear how to formulate a query that
expresses whether or not a content item is about a given topic.

In the RDF
model, it would be more appropriate to use a query like that in
Listings~\ref{lst:sparqlabout} where for a given $?article$ identified by
URI, we can get a list of URIs identifying concepts which the article mentions.
Such a query might be expressed as function $f': G \rightarrow \mathcal P(U)$ where $U$ is
set of all URIs such that:

$$
g \in G \models f'(g, uri) = \{t \: | \: (uri, about, t) \in g\}
$$

An approach of generating attributes for a given resource was proposed by
Paulheim and F\"urnkranz\cite{paulheim2012unsupervised}. They defined specific
SPARQL queries and provided case study evidence for the effectiveness of
each strategy.

Their work focused on starting with relational-style data (e.g. from a
relational database) and using \emph{Linked Open Data} to identify entities
within literal values in those relations and generated attributes from
SPARQL queries over those entities.

For a large content-producer, there is a more general problem where many content
items do not have a relational representation and the content source is a body
of text or even a raw HTML page. However, the feature generation from Paulheim
and F\:urnkranz proves to be a promising strategy given we can acquire an RDF
graph model for content items in the first place.

\section{RDF in the enterprise}
\label{sec:linked-enterprise-data}

At the end of chapter~\ref{chp:intro}, there is some discussion of the
benefits of using semantics and linked data for any process that we wish
to apply across the entire corpus.

In this section, the goal is to strengthen that case and review other
previous work around using linked data in the enterprise. This is a more
general argument for any enterprise with data (e.g. internal information
about staff or business processes) and not just media organisations.

\subsection{Data Fragmentation in Organisations}

The first thing to highlight is why enterprises struggle to work with all
the data spread across the organisation. Where all interesting data is
contained in a single database, it is relatively simple to develop software
that extracts all that data in a single query and processes it in some way.

It is rare, however, for any large business to keep all data stored in
one location and in one format. Even fairly simple businesses will have
separate data stores for their employee data, for example, and all
user names and passwords for the corporate email system. Without much effort,
a business that implements or purchases two distinct systems for personnel
and email has already fragmented its data.

There may be no difficulty from this initially, but then a problem arises
where it is noted that employees, in this example, do not automatically have
their email accounts disabled when they leave the organisation. Smaller
businesses might simple employ administrative staff to keep databases in
sync and implement a standard process to handle the case of an employee
leaving the company.

In a large organisation, it may be more desirable to automate this. This then
steps into the field of \emph{Enterprise Integration}, discussed in the next
section.

In a media organisation, it might be argued that while all employee and
internal systems data is in bespoke systems, we can ensure all ``content''
is stored in a single system such as a \emph{content management system}. This
can be strained by at least three different pressures:

\begin{enumerate}
\item diversity of content;
\item technology refresh cycles; and
\item fragmentation of teams.
\end{enumerate}

Where an organisation produces multiple \emph{types} of content, it
may be the case that one system is optimised for writing and editing textual
articles (e.g. news) and another system is purchased or created to handle
video content. If these systems are built by different teams or, worse,
are procured independently as commercial offerings, there is little incentive
for the content to be compatible for generalising techniques such as search
or data mining.

This is the compounded where development teams decide to rebuild software
systems to keep up to date with changes in the industry, implement new features
in ways not made possible by the old system or because the old system is
becoming more expensive to maintain than it is to replace. This also applies
for buying in new systems because a contract was up for renewal and the fresh
procurement process chose a competitor's commercial offering which is
incompatible than that of the previous provider.

When this happens, the teams might choose to migrate all the old content to
the system, but they also might choose not to if the older system is still
running without issues. If it can continue to serve old content in a
"read-only" state without issues, then there may not be a strong business
case to migrate that old content as long as it continues to work.

The teams may also choose to archive the old content in a flat state not
intended for editing. This allows the old system to be decommissioned, but it
does leave older content in a format not easily queries and certainly not
compatible with how newer content is actively managed.

The final problem highlighted above -- that of fragmentation of teams in the
organisation -- particularly affects very large enterprises such as the BBC.
Any media company might find that content production teams work best when
given sufficient autonomy. A consequence of this is that multiple teams
might implement very similar solutions to the same problems, but without the
incentive to share nor unify their approaches. From this, we can ultimately
end up with multiple content management systems even for the same type of
content, all actively maintained.

A lot of this falls out of what was observed by
Conway\cite{conway1968committees} (sometimes as known as \emph{Conway's Law})
in that an organisation's architecture ends up reflecting its communication
structures. From this, we can assume that if two teams in an organisation
are not encouraged to communicate well and often, then they are less likely to
share software code and data systems.

\subsection{Enterprise Integration}

The field of \emph{Enterprise Integration} is the study and application
of overcoming the fragmentation issues described in the previous
section. It is concerned with creating interconnectivity between
distinct systems, exchanging data between them and dealing with
distributed systems within the enterprise.\cite{vernadat2003enterprise}

\subsection{Organisational Difficulties with Enterprise Integration}
\label{sec:ei-difficulties}

Hohpe and Woolf\cite{hohpe2002enterprise} described a comprehensive
set of software design and architecture patterns that apply to this
field, but almost every pattern implies creating bespoke data flow
software for any given integration.

This means for every new system that is purchased or created, a
whole set of integration efforts are potentially initiated to move any
data into or out of that system.

A significant barrier to creating holistic views of all data
produced by an organisation is that each integration brings in a
vertical slice of all data required, but only for once source at a
time. If a hypothetical organisation has six content management
systems with actively-published content within them and each
integration effort takes one month, then it is a minimum of six
months before it can create something like a search feature over all
of that content.

\subsection{Linked Enterprise Data}

Allemang\cite{allemang2010semantic} proposed that a
\emph{linked data enterprise} can be truly agile by removing bottlenecks
created by enterprise integration efforts. There is an appealing
argument in using controlled vocabularies and efforts in linking
those vocabularies between systems to join up existing data silos
rather than trying to write integration layers to move data between
those silos.

A more detailed approach was outlined by Servant\cite{servant2008linking}
in how linking enterprise data might work with the view that it
could be adopted within Renault.

Real research in this area still seems sparse in the academic
literature, but there is arguably a case to be made for these
approaches for an organisations public-facing content given that is
already available on the public web.

The concept of linked enterprise data could encourage media
organisations to consider using linked data within all internal APIs
and data stores where possible such that reasoning across all that
information is more readily enabled. In this research, it is demonstrated
that we can get some value at least from the semantics available via
the content as it surfaces on the public World Wide Web, but this
value can be only increased if content producers improve the quality
of those semantics or linked data principles are used within internal
systems to reduce the need for bespoke enterprise integration effort.
