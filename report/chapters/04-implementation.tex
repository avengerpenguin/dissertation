\chapter{Implementation}
\label{chp:implementation}

In this chapter, some of the details of the system implementation
are described. Initially, we discuss some of the implementation
strategy employed due the experimental nature of the system and the
desire to run multiple, competing approaches side-by-side in an
efficient way.

The remaining sections walk through the implementation of each part
of the data pipeline from upstream to feature sets and the chapter
concludes with implementation of the clustering and generation of
clusters for evaluation.

\section{Software Architecture}

All functionality was implemented as part of a single Python module
named \emph{distillery}
that is run via the Unix command line, with different subcommands for
each feature. Each command was designed around the
Unix Philosophy\cite{raymond2003art} of doing one job per command and
that they were composable via Unix pipes. This allows pipelines to
be composed:

\begin{centering}
  \begin{lstlisting}[
      basicstyle=\scriptsize,
      label=lst:unix-pipe,
      language=bash]
distillery extract <iris.txt | distillery enrich | distillery generate
  \end{lstlisting}
\end{centering}

\noindent that resemble the entire theoretical data pipeline described
in section~\ref{sec:design-pipeline} without the need for middleware
such as message queues. More typical enterprise architectures around
message oriented middleware or event-driven architecture might need
to be employed to scale this system to millions of documents, but
the simple approach above is sufficient for tens of thousands of
documents.

Even this simple approach would scale very well with suitably low
network latency. The largest bottleneck observed was the necessity
to do at least one HTTP request per item ingested and then enrich
via an HTTP request per object of a triple where the IRI is a subject.
For some content items, this could easily be over 100 HTTP requests at
which point parallel CPU cores or asynchronous I/O programming does
not overcome the demands on network performance.

Throughput is a particular challenge when data processing systems
are first launched. The day-to-day volume of item creations and
updates may be low enough for even a low-scale application, but the
performance demands increase significantly when we wish to bootstrap
a backlog of all content items historically published -- of which
could easily be tens of millions for organisations such as the BBC.

For this, there is an
appeal to designing such a system on a cloud computing platform such
that it can be scaled up for the initial import of existing data and
then back down for ongoing updates when that import completes. Such
design is out of scope for this paper, but some discussion of
limitations of the existing system in section~\ref{} highlights where
this system may be architected differently to be more suitable for
such a cloud-based deployment.

The higher-level compositional architecture of the individual
data processing stages can, of course, be altered independently of
the design of those stages. The following sections will describe in
greater detail those composable modules.

\section{Avoiding Redundancy Across Multiple Experiments}

The design of the data pipeline in chapter~\ref{chp:design} would
suggest an implementation with each stage implemented as a function
converting a graph into a new, richer graph.

That is, whilst the different strategies illustrated in the maximal
data pipline in Figure~\ref{fig:maximal-pipeline} can be run in
parallel, the extraction stage itself simply converts the identity
graph described in section~\ref{sec:identity-graph} to a single graph
(after having taken the union of all the strategies).

In the experiment that is the subject of this paper, the intent was
to compare each of the extraction strategies individually, but also
all possible (union) combinations thereof. This presents at least
two potential implementation strategies:

\begin{enumerate}
\item Build the whole data pipeline and machine learning system such
  that it is configurable which strategies are invoked and run an
  instance per configurable combination; or
\item implement the software to produce all possible outcomes along
  the pipeline.
\end{enumerate}

The former approach is stronger if the aim is to build the system
closer to how it would be implemented in an enterprise: only one
set of clusters is needed and even a production, enterprise system
would be configurable if maintainers wished to enable or disable
features as desired.

The latter strategy is arguably better for experimentation as every
IRI fed into the source end of the pipeline is processed at the same
time for every extraction and enrichment approach. This gives better
assurance that the same input is used to evaluate each competing
system.

Other operational reasons include being able to run the system on a
single machine if necessary and that there is no repeated work:
extraction by one means can be used in isolation, but also feed into
a union with another technique without having to recalculate that
graph. That is, if we have graphs $A$ and $B$ generated once each,
then we can output $A$, $B$ and $A \cup B$ for comparison. A parallel copy
of each system would be calculating $A$ and $B$ twice each.

The limitation here is a production-ready implementation of the
system would have be built again from the ground-up, perhaps reusing
functions and library code from the experimental version, as the
application itself will be structured around this idea of multiple
outputs for each input. This is likely to be the case for any
experimental system and is arguably in line with Brooks' classic
assertion to ``Plan to Throw One Away''\cite{brooks1995mythical}.

\begin{centering}
  \begin{lstlisting}[
      label=lst:extract-all,
      language=python,
      caption={Python function that generates all possible RDF Graphs}]
def all_extracted_graphs(iri):
    
    extraction_strategies = [
        dereference, extract_entities, find_links
    ]

    # This creates all 3 RDF Graphs only once
    graphs = [s(iri) for s in extraction_strategies]

    # Empty set removed from powerset
    for idx, gs in enumerate(powerset(graphs) - {{},}):
        # union defined elsewhere: list(graph) -> graph
        graph = reduce(union, gs)
        # Loop counter idx tells us from which technique
        # combination each graph comes
        yield idx, graph
  \end{lstlisting}
\end{centering}

An illustrative Python function is shown in
Listings~\ref{lst:extract-all} where each of the three extraction
techniques is invoked only once and a powerset function is used to
generate all possible unions of those graphs and thus yield all
possible usages or non-usages of each strategy to generate RDF
graphs. The details of the three extraction functions are covered in
section~\ref{impl-extraction}.

\section{Obtaining IRIs}

It is a substantial undertaking to design, build and test a
production-quality enterprise system that processing all content
produced by a media organisation. An experimental system needs to
focus on quick results and therefore is optimised to work only over
a small sample of that content.

As introduced in section~\ref{intro-problems}, all that content is
also likely to be distributed across multiple databases, content
management systems and other stores. Whilst it is hypothesised in
this research that semantics and linked data provide a good way to
extract data about the total breadth of all content without any
bespoke integration against internal systems, there still remains
the enterprise integration problem of \emph{discovery} of those
content items.

Essentially, the data pipeline outlined in
section~\ref{sec:design-pipeline} notably requires that known IRIs of
content items are fed into it, but makes no statement about how those
IRIs are acquired. This is a deliberate design decision to decouple
the concepts of discovery from this extraction/enrichment workflow.
Thus item may easily be re-ingested several times if it is updated
(or simply periodically) and
a modular approach like this defends against upstream systems changing
and having to rebuild the triggers that notify when content is created
or updated.

What follows it that the enterprise integration task has been reduced
(we do not need to write bespoke code to extract basic data about
every content item) but not wholly eliminated (some custom adaptors
need to be created to notify the pipeline of creation or update events
in each respective data store).

However creation of any such notification adaptors is out of scope
for this research altogether, so a heuristic approach was employed
where ten different, themed content aggregation pages on the BBC
website\footnote{
  Including, among others, the BBC Homepage (http://www.bbc.co.uk),
  BBC Arts (http://www.bbc.co.uk/arts) and BBC Science
  (http://www.bbc.co.uk/science)
} were polled for any new content promoted by editorial teams.
This provided a way to find new items shortly after they are published
and also ensured the data sample ultimately used focused on content
that was deemed noteworthy or interesting in the last few months.

This is not an effective method for obtaining a large number of items
quickly as it is entirely dependent on the rate these aggregate pages
are updated. Running slowly over a long period yielded approximately
ten thousand individual IRIs with little effort nor need for large
amounts of integration work.

\section{Extracting RDF Graphs}
\label{sec:impl-extraction}

Listings~\ref{lst:extract-all} hinted at three individual functions
capable of obtaining an RDF graph for a given IRI, all based on the
extraction strategies outlined in section~\ref{sec:rdf-extraction}.

\subsection{Dereferencing}

Listings~\ref{lst:deref-simple} shows how trivial a dereference
function can be if we use the comprehensive RDFLib library for
Python. The library can handle content type
negotiation\cite{fielding2014hypertext} to ensure it can happily
dereference and parse any response that contains semantics,
particularly RDFa (including embedded Turtle) and microdata within an
HTML page.

\begin{centering}
  \begin{lstlisting}[
      label=lst:deref-simple,
      language=python,
      caption={Python function that generates all possible RDF Graphs}]
from rdflib import Graph
    
def dereference(iri):
    g = Graph(identifier=iri)
    g.parse(iri)
    return g
  \end{lstlisting}
\end{centering}

\subsection{Entity Extraction}
\label{sec:impl-entity-extraction}

DBPedia Spotlight\cite{isem2013daiber} was chosen to perform the
entity extraction. A hosted version is available as a free service and
it has been shown to be effective in many cases. A true comparative
study of the merits of alternative entity extractors is out of scope
for this paper.

\begin{centering}
  \begin{lstlisting}[
      label=lst:extract-entities,
      language=python,
      caption={Python function that uses DBPedia Spotlight to extract entities from web pages}]
import requests
from rdflib import Graph, URIRef, Namespace

FOAF = Namespace('http://xmlns.com/foaf/0.1/')
    
def extract_entities(iri):
    g = Graph(identifier=iri)

    text = requests.get(iri).text
    
    spotlight_response = requests.post(
        'http://spotlight.sztaki.hu:2222/rest/annotate',
        data={
          'text': text, 'confidence': 0.8, 'support': 20
        },
        headers={'Accept': 'application/json'},
        timeout=600)

    if spotlight_response.ok:
        r_json = spotlight_response.json()
        if 'Resources' in r_json:
            annotations = {
                resource['@URI']
                for resource in r_json['Resources']
                }
            if annotations:
                for entity in annotations:
                    g.add((
                        URIRef(iri),
                        FOAF.topic,
                        URIRef(entity)))
    return g
  \end{lstlisting}
\end{centering}

Listings~\ref{lst:extract-entities} shows a basic implementation of
a function that fetches a page and then feeds the content into a
request to DBPedia Spotlight. All entities thus found are then fed
in as objects to a series of \texttt{foaf:topic} triples.

There is much that can be tweaked in this implementation and the
final version used in the experiment relied on a ad hoc whitelist
of XPATH queries known to narrow down the content to the main article
content on BBC pages. This is because the implementation shown in
listings~\ref{lst:extract-entities} does nothing to prevent entities
being extracted from page
chrome such as navigation links and other cross-site concerns.

In a given organisation, such an approach might well be sufficient to
extract main article content from a controlled set of pages. In a
production system, it may be more appropriate to consider
readability systems such as that from Arc90.

\subsection{Hyperlink Relationships}

In listings~\ref{lst:extract-entities}, we see a simple Python
function that uses the Beautiful Soup
library\footnote{https://www.crummy.com/software/BeautifulSoup/} to
search across all links in a document and infer a ``related''
property on the assumption that pages like to other pages that are in
some way related.

\begin{centering}
  \begin{lstlisting}[
      label=lst:find-links,
      language=python,
      caption={Python function generates triples based on links to other pages}]
import requests
from rdflib import Graph, URIRef, Namespace

DBPROP = Namespace('http://dbpedia.org/property/')
    
def find_links(iri):
    g = Graph(identifier=iri)

    text = requests.get(iri).text
    doc = BeautifulSoup(text)

    # For all <a> tags that have an href attribute...
    for a_tag in doc.find_all('a', attrs={
        'href': re.compile('.+')
    }):
        # ... handle href values being relative links ...
        other_iri = urljoin(iri, a_tag.attrs['href']).strip()

        # ... infer triple if IRI is to another BBC page
        if bbc_url.match(url):
            g.add((
                URIRef(iri),
                URIRef(DBPROP.related),
                URIRef(other_iri),
            ))

    return g
  \end{lstlisting}
\end{centering}

This falls foul of similar issues to entity extraction as described
in section~\ref{sec:impl-entity-extraction} in that site-wide
navigation links and other supporting page elements may link to
very general pages (e.g. a persistent link to ``home'' on every page).

With appropriate feature selection, this may have no impact, but for
this experiment a simple heuristic was employed to narrow down to
the ``interesting'' part of BBC pages.

Another feature in listings~\ref{lst:find-links} is a restriction
to consider only links to other BBC pages. There may be a positive
or negative effect in addtionally noting external pages linked to from
content pages, but that is left to future research to consider.

\section{Enriching and not Enriching}

The combinatorial consequence of the usages and non-usages of the
three extraction techniques described so far is that the
experimental system developed is already comparing seven ($2^n$ less
the case where no extraction is employed).

In order to maintain focus in the experiment, only one of the
enrichment techniques described in chapter~\ref{design} was
implemented and its usage and non-usage merely doubled the
number of systems for comparison to fourteen.

This proved sufficient to give indicative results as to whether there
is value added through any enrichment, but future research could
certainly compare the merits of different enrichment approaches.

\begin{centering}
  \begin{lstlisting}[
      label=lst:enrich,
      language=python,
      caption={Python function that enriches a graph via deferencing objects}]
import rfc3987
    
def get_objects(graph):
    return {str(row.o) for row in graph.query('''
            SELECT DISTINCT ?o
            WHERE {
              ?iri ?p ?o .
            }
            ''', initBindings={'iri': graph.identifier})}

def enrich(graph):
    new_graph = copy(graph)

    for o in get_objects(graph):
        if rfc3987.match(o, rule='IRI'):
            new_graph.parse(iri2uri(o))

    return new_graph
  \end{lstlisting}
\end{centering}

Listings~\ref{lst:enrich} illustrates a simple function to
deference all objects of triples where the current IRI is a subject.
Note the convention to set the graph's own identifier to that of
the content item of interest.

Using SPARQL in this way opens up possibilities to perform multiple
queries to choose IRIs to dereference. For example, we may wish to
include semantics for objects reachable by following two predicates
on the RDF graph and consider much more indirect data. Evaluating
the utility of this is left for future work.

\section{Feature Set Generation}

The feature generation strategy employed was dervied from a distilled
portion of the approach introduced by Paulheim
and F\"urnkranz\cite{paulheim2012unsupervised}.

Listings~\ref{lst:generation} shows an illustrative, recursive
function that ``walks'' the RDF graph starting with the content
item's IRI as the initial subject.

\begin{centering}
  \begin{lstlisting}[
      label=lst:generation,
      language=python,
      caption={Python function that generates feature sets from RDF graphs}]
from rdflib Literal, URIRef
    
def generate(g, subject=None, depth=3, features=None, prefix=''):
    if depth <= 0:
        return
    if not features:
        features = {}
    if not subject:
        subject = g.identifier
    for p, o in g.predicate_objects(subject=subject):
        if type(o) == URIRef:
            new_prefix = prefix + clean(p) + '_'
            features[new_prefix + clean(o)] = True
            generate(g, depth=(depth - 1), features=features, prefix=new_prefix, subject=o)
        elif type(o) == Literal:
            features[clean(p)] = str(o)

    return features
  \end{lstlisting}
\end{centering}

The \texttt{clean} function simply provides some cosmetic cleaning
up of predicate IRIs to prefer CURIEs as they are more compact. A
typical example feature might then be
\texttt{foaf:topic\_dbpedia:Albert\_Einstein: true} where the leaf
object is an entity or
\texttt{ogp:title: "Gandhi: Reckless teenager to father of India"} if
the leaf object is a literal.

\section{Feature Selection}

A simple selection heuristic was employed, again inspired by the work
of Paulheim and F\"urnkranz\cite{paulheim2012unsupervised}, to go
some way to reduce potential noise in the subsequent machine learning
phase.

In this experiment, all features that have the same value or entirely
unique values were removed and values that occurred twice or more
for a given feature had to make up over 5\% of possible values for
that feature. That is, if a feature had over 95\% of its values being
unique or all the same, then it was rejected.

\section{Clustering Implementation}

Hierarchical, agglomerative clustering was implemented by implementation
of a \texttt{cluster} function that was merge the two ``nearest''
clusters on each invocation. This assumes that all content items not
yet in a cluster are implicitly in a singleton cluster and the ability
to merge one at a time allowed for control of when to cease merging.

The closeness of two potential, candidate clusters was calculated via
a the complete linkage function as chosen in section~\ref{sec:clustering-media}.
This made use of an implementation of the Jaccard distance as also
justified in that same section.

\section{Results Generation Strategy}

With all of the components described so far in place, an experiment
was run processing nearly 10,000 content items from the BBC website
with each going into one of fourteen combination of extraction
and enrichment approaches.

For each approach, hiearchical clustering was applied repeatedly
until the next merge would produce a cluster with cohesion below 0.5.
This is reasonably arbitrary as each technique will have different
concepts of cohesion, but it provided a strong basis on which to
evaluate each approach on whether its cohesion meaningfully predicts
whether a human would agree that cluster contains related items.
