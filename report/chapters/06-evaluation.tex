\chapter{Evaluation}
\label{chp:evaluation}

This chapter builds on the more immediate analysis in
chapter~\ref{chp:analysis} of the clusters produced by evaluating the
results across a group of people. The system designed and built is
also a subject of evaluation, particularly any flaws or potential
improvements that emerge from having run the software and reviewed its
results.

A higher-level
goal of this research was to perform some more qualitative evaluation
of those clusters by obtaining human feedback on a potential
application thereof. The application evaluated was a feature that would
take a page a user is currently reading and suggest related content
based on their being members of the same cluster. Mock-ups of such
a feature were generated and evaluated via a survey as described
in section~\ref{sec:survey}.

The remaining sections of this chapter then discuss some issues with
the design and difficulties arising during implementation
respectively.

\section{Qualitative Survey}
\label{sec:survey}

An online survey was produced where a user is shown a sample page
from the BBC website and for each cluster of which that page is
a member, a row of up to four ``suggestions'' was presented to users.

In this section, some details around the design and sampling criteria
for the survey are discussed initially, followed by analysis of the
results. The section concludes with further qualitative feedback and
observations that arose in survey responses.

\subsection{Survey Design}

An initial challenge for generating the survey was that it was
necessary to present to users a view starting from a particular
content item. This as effectively an inversion of the analysis
from chapter~\ref{chp:analysis} where a top-down overview of the
clusters was performed, with some drill-down into more depth in some
cases.

A reasonably arbitrary sample size of 20 content items was chosen
for which it was therefore necessary to choose 20 \emph{pages} that
were known to be members of one or more clusters. It was also
desirable to ensure that multiple clusters are evaluated in one
survey question, so items that were placed into clusters by multiple
instances of the data pipeline were preferable.

It was also important to ensure that the overall set of items
chosen provided coverage across all permutations of the data pipeline
and that some diversity of content was present (e.g. all items are
not simply news articles without any representation of TV and video
content).

A further additional property chosen for the sample set was that
a diverse range of cluster \emph{cohesion} values were present in the
set. This arguably allows the survey to present for evaluation
clusters that the machine learning applied itself believes to be poor
as well as ones it believes to be cohesive. This controls
for coincidences where only good clusters were chosen for one approach
and another had its least cohesive results chosen. It also has
potential not just to compare which approaches receive the most
relevance votes, but also to look for where those approaches' results
produce clusters whose cohesion measure correlate with anything
meaningful to humans.

The above sampling heuristics were applied programmatically by first
inverting the cluster data structures such that there is a known
list of all content IRIs across the whole results set and the list
of clusters of which they are members are keyed against those IRIs.

A composite uniqueness key was then created for each membership those
content IRIs have in the form:

$$
u(iri, cluster) = (approach, cohesion(cluster), category(iri))
$$

\noindent where $approach$ is a unique key referring to a particular
permutation of the data pipeline being evaluated and $category$ is
a rough, heuristic function that tries to assess which part of the BBC
website ``owns'' that content item, e.g. BBC News, iPlayer, Weather,
Sport. Each item would have a uniqueness key attached to it for each
cluster of which it is a member.

Content items were initially scored based on how many clusters of
which they were a member:

$$
score(iri) = 10 - |5 - \left\vert{\{c \in C \: | \: iri \in c\}}\right\vert|
$$

\noindent where $C$ is the set of all clusters produced. This gives
us a score of 10 when the number of clusters in which the item appears
is the ideal value of five. The justification for this optimum is
based on the lower end of the memory capacity limits suggested by
Miller\cite{miller1956magical} (Miller's Law).

The item is thus ``punished'' when it appears in only one cluster
(thus the question would only be testing one approach and has less
value as a question) or if the item appears in too many clusters
(asking people to compare between too many items might be
overwhelming and reduce the potential for people to answer
accurately). As this is only a heuristic, the sampling code still
chose some items nearer this extremes, thus an absolute cap was
implemented in the survey itself to ensure a random sample of seven
clusters was chosen for evaluation when items appeared in a number
greater than seven.

Starting with the highest scoring so far, the sampling algorithm
then updated each score based on the number of unseen unique keys
it would introduce to the final sample if it were chosen:

$$
score'(iri) = score(iri) \times \frac{15 - |keys(iri) \cap K|}{2}
$$

\noindent where $keys(iri)$ is all uniqueness keys for an IRI:

$$
keys(iri) = \bigcup_{c \in C} u(iri, c)
$$

\noindent and $K$ is the cumulative union of all keys so far
generated by invocations of of the $keys$ function so far, with its
initial value being the empty set.

Some of the constants used were tweaked based on what appeared to
generated a desirable, mixed set of sample results such that they are
a little better than arbitrary values. Once all IRIs were updated with
the new scores, the final sample of
results for evaluation was simply the content items with the twenty
highest scores.

\subsection{Survey Results}

The survey was taken by 30 respondents with three having taken it
following the think-aloud protocol\cite{lewis1982using} so that more
information can be gathered around the motivation for the responses.
Two of the think-aloud participants were experts in usability testing
and user experience design and the remaining participant a normal
website user.

Details of observations captured during the think-aloud responses
are discussed in section~\ref{sec:eval-obs}.

A key metric generated from the responses was the proportion of times
respondents selected a ``related'' set of items based on clusters
generated from a particular data pipeline variant. A table of the
responses is given in table~\ref{tbl:results}.

\begin{table}[h]
  \centering
  \caption{Success rate for each pipeline variant in generating related content suggestions}
  \label{tbl:results}
  \begin{tabular}{lll}
    & Not enriched & Enriched \\
    Deference                                     & 5\%          & 44\%     \\
    Entity Extraction                             & 17\%         & 37\%     \\
    Hyperlink Relationships                       & 8\%          & 14\%     \\
    Dereference and Entity Extraction             & 34\%         & 37\%     \\
    Entity Extraction and Hyperlink Relationships & 16\%         & 11\%     \\
    Dereference and Hyperlink Relationships       & 26\%         & 30\%     \\
    All three extractions                         & 23\%         & 24\%     
  \end{tabular}
\end{table}

A tabular form of the results is helpful for showing which techniques
performed the best and whether enrichment improved or worsened the
performance. However, it is difficult to derive absolute meaning
from numbers taken from qualitative feedback over such a small
sample.

\begin{sidewaysfigure}
  \begin{center}
    \includegraphics[width=\linewidth]{diagrams/results.png}
  \end{center}
  \caption{Visualisation of performance of each data pipeline variant\label{fig:results}}
\end{sidewaysfigure}


In figure~\ref{fig:results}, each pipeline variant is visualised
as a node in a directed graph. Note the nickname RDFa to represent
dereferencing and extracting embedded semantics despite RDFa being
only one of a few semantic markups extracted in this case. Also note
the convention to suffix the variant name with $+$ to indicate it
has been enriched. The nodes
on the far left represent each of the three extraction techniques
used in isolation.

The edges of the graph represent introducing one new technique and
re-running the experiment. For example, the edge from \emph{RDFa} to
\emph{RDFa+} shows what happens to the performance if we introduce
enrichment via dereference to data extracted also by dereference.
The edges from e.g. \emph{Entities} and \emph{Links} point to
the result of having used both of those techniques in data extraction
(joining the graphs via set union).

Where a percentage increases (significantly), we can infer with some
level of confidence that adding in a particular technique
\emph{improves} the performance of the overall system for choosing
clusters of related content. Conversely, where the percentage drops
a large amount, we can infer that introducing that technique has
worsened the performance.

In the particular results depicted, there are a mix of insignificant
and significant changes in performance when we alter one aspect of
the system. Insignificant results include:

\begin{itemize}
\item In all but one case, the enrichment via dereference had
  very little effect. Changes between 1\% and 5\% do not stand out
  as large jumps in behaviour, particular with the small sample size
  of people being likely to make the error margins quite large.
\item Extraction via entity extraction does not seem to improve nor
  degrade when adding in hyperlink relationships.
\item The combination of dereference and hyperlink relationships
  does not perform very differently when entity extraction is added.
\end{itemize}

Some of the most significant results appear to be:

\begin{itemize}
\item Extraction via dereference only improves substantially when
  improved in \emph{any} way. It benefits most significantly out of
  all variants from enrichment.
\item Entity extraction benefits somewhat being combined with
  embedded semantics obtained via dereference, certainly more than
  it benefits from being combined with hyperlink relationships.
\item Hyperlink relationships perhaps benefit a little from
  enrichment (but it might be bold to claim that), but more significantly
  we can see that extraction technique in performance when combined
  with either of the two other extraction approaches (or both).
\end{itemize}

The step change between embedded semantics in isolation and the
enriched version thereof is so large it prompts a brief return to
reviewing the clusters each variant produced.

If we refer back to table~\ref{tbl:cluster-counts}, we can see
another significant step change in that the unenriched variant
produced 5 (fairly large) clusters and that number drops to 249 when
enrichment is introduced.

Considering also that this extraction approach performs the most
poorly across all results, we begin to see support for the analysis
in section~\ref{sec:anal-deref} where it was observed that only very
basic metadata is found embedded in many BBC pages. This appears to
have created a sample of feature sets that have a very small number
of features each. Further observations in
section~\ref{sec:anal-deref} stated that
many of the semantics found had
common values (e.g. \texttt{ogp:site\_name="BBC"}). That many features
thus did not pass feature selection (since they did not appear
with enough value variation across the corpus) means that the few
semantic properties found are stripped down even further.

Much of the performance gain by altering extraction via dereference
might be explainable purely due to having added more data properties
with which the machine learning algorithms can work. This starts to
suggest a potential for a minimum feature set size below which
clustering processes will struggle.

This is especially supported by
the large performance increase from having simply enriched the data.
Given the enrichment in a lot of cases just added more higher order
properties describing the semantic properties themselves (e.g.
\texttt{md:item\_rdf:type\_rdf:List} appeared in some cases, which
tells us only that the \texttt{md:item} property found takes
list objects as values), we can safely conclude much of the
``enrichment'' has not added information about the content, but
just increased the triple count enough to make the clustering more
effective.

Given the noise from page structure and meta-semantics like RDFS and
OWL, it might be surprising that embedded semantics performed well
at all. The strength that appears to have had the most impact is that
the few times semantic properties are used in BBC pages, content
producers are including basic categorical information such as what
part of the BBC site we are on (e.g. \texttt{ogp:site\_name="BBC Sport"})
and in some cases which section within that site
(e.g. \texttt{article:section="Golf"}).

The basic categories seemed to be powerful enough to please many
respondents who frequently approved of related content suggestions on
the basis that the suggestions were from the same part as the BBC
website. This may be an artefact of framing the survey in an
artificial way as people validating the system by whether the pages
are grouped by well-understand categories (e.g. football news, comedy
TV, science articles) does not necessarily correlate with how those
same users would have been tempted to follow links to suggested items
as part of their own casual browsing of the website. This point
was also raised in some of the think-aloud discussions with user
experience experts, as discussed in section~\ref{sec:eval-obs}.

It seems reasonable to conclude that embedded semantics perform
well enough to produce clustering that appears intelligent on casual
inspect. Given that it is one of the simplest to implement (run
a standard RDFa and Microdata parser such as RDFLib in Python), there
is much to say for its cost-benefit ratio. It could be concluded this
is a strong starting point for a first iteration of a real implementation
of this experimental system.

Entity Extraction performed most strongly out of all extraction
techniques in isolation. This result is even more promising when we
consider that the parameters of the DBPedia Spotlight extraction used
were arbitrarily chosen, so there is certainly scope for further
work to find better performance with better tuning of the entity
extraction alone.

The favourable discussion of both embedded semantics and entity
extraction above is supported somewhat by two observations in the
results visualisation: their combination appears to perform best out
of all pairings of extraction techniques and adding hyperlink
relationships to that pair reduces the performance. The joint
occurrence of these two facts may not be coincidental and thus
support the short analysis in section~\ref{sec:anal-hyperlink} where
even brief review of the clusters provides little confidence in their
utility. Some potential ways to improve this extraction were given
at the end of section~\ref{sec:production-recommendations}, but from
this research alone we cannot conclude any benefit in extracting
relationships in this way until those improvements are trialled.

It is also important to note conclusions from the lack of significant
step changes and the most interesting one in these results is the lack
of support for enrichment via dereference improving performance at
all. This follows on from analysis in section~\ref{sec:anal-enrichment}
that suggests the use of dereference simply falls down on the same
drawbacks as dereference as an extraction technique. The only case
where enrichment makes a large difference is improving extraction via
dereference, but this could be explained more by the latter performing
so poorly in the first place, as discussed above.

In summary, there is much promise to consider starting with
embedded semantics where the system will do little more than learn
categorisation of content (in the case of BBC pages, that is) and
look to bring in entity extraction in a controlled way so that
the tuning of the extraction can be experimented with.

A production version of this system should aim to achieve an evolution
from finding the ``obvious'' distinctions from the content (i.e.
do we really need machine learning to tell us that BBC Sport articles
are related to each other, but distinct from BBC Food recipes?) to
finding more nuanced relationships based on entities and topics found
within the content if it is monitored and tuned well. This difference
between clear-cut differences and more topic-based relationships
turned out to be a key theme from the think-aloud evaluation and
consequent discussions with user experience experts. This is
discussed in section~\ref{sec:eval-obs}.

\subsection{Further Observations and Discussion}
\label{sec:eval-obs}

Further qualitative feedback was obtained in two ways: firstly by
adding to the end of the survey some open-ended questions to invite
some discussion:

\begin{itemize}
  \item Do you feel that suggested links to promoted content is something you would use in general? Feel free to expand on your answer.
  \item Do you have any general observations on where the suggestions performed poorly or where they performed well?
  \item Please write any other comments or observations below.
\end{itemize}

Approximately half of respondents answered the first two and only
three out of the 30 went on to give some additional comments. Many
of those were user experience experts who felt they had much to
contribute additional feedback.

The second phase of additional, qualitative feedback was notes taken
during the ``think-aloud'' survey responses as described in
the previous section.

In the think-aloud observations, non-expert responses were terser
than those from experts and it seemed as though conclusions were
reached more quickly about
whether suggested content was relevant or related. The impression was
that non-experts may act on some instinct that is hard to
describe, whereas experts in user design are more experienced in
some of the rationales and patterns they have observed across many
user testing sessions. This more instinct-driven approach for
non-experts is supported further from observations that thumbnail
images were cited on more than one occasion as a motivator for
determining content as being similar or not.

The expert think-aloud observations highlighted the key difference
between content being similar due to being in the same categorisation
groups versus being related by themes or topics, as discussed at the
end of the previous section. The opinions from these user experience
experts was that a topic-based similarity would be strongest for users
to consider clicking through to the suggested content and this was
vindicated on a number of occasions in non-expert responses two
(in one example a respondent was disappointed that many suggestions
offered next to a BBC News article were just ``more articles'' and
not ``articles on the same topic'').

The topic-based feedback gives more support for getting good quality
metadata \emph{about} the content in far more fined-grained and
nuanced ways than broad categories typically found in the embedded
semantics in BBC pages. Entity extraction looks to be a strong
candidate to support this if we cannot get more nuanced topic tags
into the embedded semantics. This case is supported well by the
analysis of the main responses in the last section.

The expert discussions raised some cases where ``hard'' categories
for the content were necessary, however. In some cases the clustering
processes had grouped content suitable for children with content not
explicitly marked as such. For a media organisation like the BBC, where
there is wide provision of content targeted at children, any
promotional feature on the website needs to be very cautious about
what it suggests. In the BBC, it is typical to create a strict
separation between child and non-child content and a general-purpose
data extraction pipeline has no automatic knowledge of this. In a
real setting, it is likely a production version of this system would
want to have some basic business rules to constrain its operation
where these strict requirements exist.

Another case for hard categorisation came from BBC Bitesize pages
with GCSE revision material for 14 to 16 year old British children.
There was some approval where it suggested revision material
for other subjects (relating GCSE French to GCSE English, for
example) and even when it appeared to suggest other content aimed
at a younger audience (e.g. BBC Newsround news articles aimed at children),
but it was raised whether it was appropriate to ``distract'' children
revising for GCSE exams with entertainment content. This highlights
more consideration for business rules.

It was also noted
that GCSE revision material was sometimes grouped with material for
even younger age groups (e.g. Key Stage 2 for 11 year olds) which
is unlikely to be helpful. This is another drawback of trying to
``learn'' relationships without prior knowledge of strict categorisation
of the UK National Curriculum structure. The BBC has however modelled
this as a semantic ontology\footnote{http://www.bbc.co.uk/ontologies/curriculum}
which may be promising for future iterations of this research where
we can incorporate more inference and enrichment based on
ontologies.

The final case where strict separation of content is needed is where
the system chose to cluster many British football teams together.
Semantically, the system was behaving very well in identifying that
all pages are published by BBC Sport and talk about a football team,
its fixtures and its results. However, with proper domain expertise,
a system should know not to suggest arbitrary, alternative football
teams to a user who is on the page for their favourite team. It is
unlikely they wish to read about teams they do not follow.

One last point raised in discussions with user experience exports was
how they believe that users on a given part of the BBC site may be
in different moods or have different goals depending on where they
are. For example, users reading information and educational guides
are believed to be more likely to be in a position to be suggested
other informational content even if it's only tangentially related
because their focus will just be on reading interesting things.
Conversely, people reading articles on BBC News might only wish to
be suggested articles on the same topic or developing story.

These different moods or goals seem similar to some of the search
and discovery modes proposed by Russell-Rose, Lamantia
and Burrell\cite{russell2011taxonomy}, where they try to give names
to different modes of behaviour exhibited by users when using search
or navigation systems to find information. It is not immediately clear
how we can infer these different modes or use cases based on
the semantics of different content items, but there could be some
consideration to how knowledge of user behaviour can feed in to
any machine learning processes.

There is much useful feedback and discussion raised just from
discussion with some user experience experts and also non-experts as
they evaluated the system's output in the survey. The open questions
at the end of the survey prompted a large number of comments, which
can be summarised as:

\begin{itemize}
\item Some liked where clear categorical grouping had occurred, e.g.
  news articles grouped with news articles, perhaps from the same
  category. Areas of the site where clear categories are lacking
  from the embedded semantics were mentioned as performing less well.
\item Others were less happy with suggestions being driven by
  broad categories only and felt they needed to relate by topic
  before they would follow the links.
\item There were a range of opinions as to whether the results
  were overall good or bad.
\item Two or three comments supported the idea that more weakly
  related content was better on the information sites, also supporting
  the idea that users are in a more exploratory ``mode''.
\item At least one comment noted that content was old. The clustering
  has no knowledge of what is current and what is older (which is
  very important for things like news).
\item Comments generally supported the idea of having a related
  content feature at all, but many were quick to add that it had
  to be truly relevant otherwise they may ignore it in time or treat
  it as if it were spam or advertising.
\item There were many comments, even from BBC employees about
  having discovered parts of the BBC site they were not previously
  aware of. Some of this was helped by the sampling heuristic
  deliberately choosing a diverse range of content for evaluation, but
  it does support the idea that users might need features driven by
  data in this way to suggest parts of the website they did not
  know about before.
\end{itemize}

A common theme from both this section and the previous section is
this balance between structured, categorical data and topic and theme
information both feeding into how content is grouped. This provides
strong evidence again for the benefits of combining structured information
from embedded semantics with fuzzier approaches like entity extraction.

The simple survey designed and given to only 30 people has raised
a substantial amount of feedback on the variants of the system. In
the next section, there is some evaluation of the software itself
from a design perspective and some discussion of some of its
shortcomings that a production version might wish to avoid.

\section{Design Limitations}

Throughout chapter~\ref{chp:design}, the design of the data pipeline
and overall software architecture was outlined and known trade-offs
and limitations were highlighted. Further design trade-offs due to
the systems intended use to be experimental rather than
production-quality in an enterprise were also covered.

This section highlighted some design issues that were noted only after
the system was built and how research and industry alike might learn
from these problems. Such problems might be addressed if the results
of this experiment are to be researched further or indeed if such
a system were to be built in a real media organisation. Such
learnings for production use should follow on from the
recommendations in section~\ref{sec:production-recommendations} around
applying the data extraction techniques used and provide more general
software design recommendations.

\subsection{Performance}
\label{sec:eval-perf}

The experimental system ran far slower than would be acceptable in
an enterprise system and even to the extent that it would limit
developing this research to the scale where it is processing millions
of items -- a target that would be ideal for further proving the
suitability of semantics and machine learning on real media
content corpora.

A better-performing system might have allowed even this experiment
to consider larger numbers of items, although even a relatively
small sample size should sufficiently indicate which techniques look
more promising for developing a more efficient system that focuses
thereon (and save time by dismissing techniques that show no promise).

The performance difficulties that arose from the design and concept
broadly break down into two problems:

\begin{enumerate}
\item lack of consideration for data structures for the feature sets
  and clusters; and
\item network latency bottlenecks.
\end{enumerate}

The first problem stems from the fact that the design was precise
around the in-flight data in the pipeline, less consideration was
given for how to store the resulting feature sets and also the
clusters. The feature sets were stored as simple JSON objects on
files on disk, which is perfectly fast for writing and reading, but
there was no pre-calculated distance matrix between those objects. The
clusters were also stored in a na\"ive fashion of larger JSON objects
that embedded their members' data for quick display of those clusters
and calculation of things like cohesion.

A distance matrix is a $O(n^2)$ operation in terms of computational
complexity, which scales poorly, particularly for a continuous ingest
of data where each new item invalidates the matrix. The design of
the system implemented in this research was built around an assumption
that we wish to have a continuously-running data pipeline that can
update as new content is published. Recalculating the distance matrix
on each new item would effectively push the complexity up to $O(n^3)$
which is clearly undesirable.

With this in mind, a cut-off for content ingest had to be determined
so that the distance calculations and then the clustering could be
invoked. The system implemented does not save any intermediate data
such as distances, so adding in a single new item or cleaning out
any duplicates found required all the results to be regenerated from
the start.

A production version of this system would have to look at a distance
matrix and cluster set structure that allows for real time updates
of new and updated content, efficiently changing only parts of the
matrix affected (e.g. a new item only needs to calculate its distances
with each existing item and does not affect existing pairs). This
could be a promising direction for further research.

The other issue of network latency became very apparent in particular
when enrichment via object dereferencing was enabled. Whereas an
enterprise integration effort between two different data stores
operates on bespoke software engineered in a controlled network,
extracting RDF data via a URL dereference is susceptible to network
latency over the public Internet. Also, HTML pages with embedded
semantics have a very poor ratio of semantic metadata to irrelevant
information -- a document DOM focuses a lot on marking up the content
for web browsers.

There is still a compelling case for the \emph{decoupling} achieved
by using standards like RDFa and the \emph{abstraction} HTML and HTTP
provide over the underlying data stores, reducing the need for
bespoke clients. However, any extraction of that data needs to
tolerate or factor in the increased latency for this approach.

For extraction, the latency of a single HTTP request (or two or three
if HTTP redirects are issued) will only cause a bottleneck if new
content is published or updated at a rate faster than the pipeline
can process. This seems unlikely even in an organisation like the BBC,
particularly if the system implements structures such as queues to
handle bursts of input, processing them in quieter intervals.

However, for enrichment via deference, an RDF graph produced might
have hundreds of objects needing dereferenced via HTTP request. A
simple cache layer might prevent the system repeatedly requesting for
the same URL in a short period, but this kind of latency still means
it could take of the order of minutes for a single item to get
through the pipeline.

It is not clear that there is a single answer to handle this as it is
likely a combination of factors need optimised to make this form
of enrichment acceptable:

\begin{itemize}
\item It might be that hundreds of objects linked via predicates
  creates too much noise in general. If those objects have come mainly
  from entity extraction, should entity extraction be tuned to bring
  back only the ``main'' topics of a page?
\item Chapter~\ref{chp:analysis} discussed how many of the extraction
  techniques (particularly deference) produced a lot of irrelevant
  facts (e.g. about the page itself, not the content) all of which
  would be candidates for deference of the objects are IRIs.
  Effectively filtering out such noise would reduce the work spent
  enriching irrelevant things further.
\item A simple architectural solution is to parallelise the HTTP
  calls as much as possible such that an item's latency is only as
  long as the slowest response time. There are obvious limits to this
  (we should avoid overloading the server even if it is scaled for
  millions of visitors per day), but any parallelism is clearly
  better than a hundred sequential HTTP requests. Parallelism includes
  distributed architecture over multiple servers as much as
  concurrent programming paradigms.
\item An organisation should probably ensure a system is physically
  near any servers serving HTTP requests to reduce the overall TCP
  packet latency. This optimisation will add up over time, but is
  a challenge for a large organisation such as the BBC with systems
  of varying ages distributed over multiple locations
  \footnote{Industry has
  long outgrown the notion
  that enterprises like the BBC have a single ``www'' server as
  in the early days of the World Wide Web. However, the www hostname
  still needs to resolve to a single set of IP addresses for something
  that fronts the rest of the network like a load balancer. Effective
  caching in this front layer and ensuring our data pipeline lives close
  to that cache might be sufficient in this case. An example of a
  front cache might be something like the Varnish HTTP cache:
  https://www.varnish-cache.org/}
\end{itemize}

Even with all the optimisations possible, it is inescapable that
multiple HTTP requests to assemble an RDF graph is slower than a
single query to a data store's query engine (e.g. SQL) that returns
all the information needed in a single response. The benefits of using
semantics, HTTP and other standards -- such as decoupling, simple
enrichment, reuse of standard technologies -- need to be effectively
researched and promoted, otherwise enterprise organisations will
continue to reach for the seemingly obvious solution of building
software that queries one database to push to another.

\subsection{Noise}

The semantic web and more recently Linked Data are arguably built
around the idea that they embrace the loose architecture of the
World Wide Web. With that looseness -- open world assumption,
no schema constraints, etc. -- we cannot assure strict quality and
integrity of the data as would be possible in a closed-world
database.

The data pipeline design in chapter~\ref{chp:design} has no
requirements nor consideration for how to handle facts that are
erroneous, missing nor meaningless (e.g. see the discussion
in section~\ref{sec:anal-deref} of the system finding facts about the
pages' structure rather than content metadata).

In section~\ref{sec:impact-of-noise}, it is argued that noise might
not have as much impact as one would assume given the fuzzy nature
of machine learning in general. This argument is important to consider,
but here we explore the issue of noise on the assumption that it
\emph{is} a problem as it is important to evaluate how noise fed into
the system in the first place and how we might tackle it.

Erroneous facts may be non-trivial to distinguish from real facts, but
there is scope for research on effective methods for reducing that
noise. In an enterprise, we need not exclude the benefits of
enterprise integration development to \emph{support} this approach
based on semantics and web standards.

While there is much gained
from being able to reason about the entire breadth of an organisation's
content using semantics, it does not preclude writing bespoke business
rules that strip out known, erroneous data or provides a framework
against which to evaluate the likelihood of a fact being useful or
true. At the very least, there could be some work to develop ontologies
and schemas that detect inconsistencies in the data, but in a way
that adds value to a working system rather than such modelling being a
barrier to building the first iteration of the system.

Facts that are missing are arguably not a ``noise'' issue, but still
a data integrity concern. If an item lacks key information, it may
be categorised incorrectly. It should be noted that the data pipeline
designed here did nothing to address this deliberately and the desire
was to evaluate the open world characteristics of semantics without
trying to fill in the gaps.

As with the erroneous facts discussion above, there is much case to
be made for designing such a system so that it is extensible enough
with customisations and bespoke software to aid where it appears to be
struggling -- once its initial behaviour is evaluated. In the case
of the data pipeline designed in chapter~\ref{chp:design}, any
in-house databases and APIs can be integrated as additional extraction
techniques to be joined via union to more general approaches such
as dereferencing.

Data being irrelevant might be the larger of the three concerns here
as that greedy machine learning algorithms can get stuck on such
noise. In section~\ref{sec:few-large}, we see how one approach
caused the clustering to get stuck on the fact that all items linked
to a common FAQ page.

Other data attributes such as the Twitter
Cards metadata (see section~\ref{sec:smallest-cluster}) can contain
very broad things that are declared on all pages that use this
vocabulary (e.g. \texttt{card=summary} is always used to indicate that
a given page is using these meta tags) might cause a greedy algorithm
to focus too long on grouping together all pages that happen to choose
to implement this Twitter feature with less concern about the more
interesting properties.

