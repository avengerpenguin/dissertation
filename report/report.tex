\documentclass[10pt,a4paper]{report}

\title{Improving content discovery through combining linked data and data mining techniques}

\author{Ross Fenning}

\usepackage[dvipsnames]{xcolor}
\usepackage{nag}
\usepackage{rotating}
\usepackage[autosize]{dot2texi}
\usepackage[Bjarne]{fncychap}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,decorations}
\usepackage{listings}
\usepackage{textcomp}
\usepackage{mathtools}
\usepackage{caption}

\DeclareCaptionFont{white}{ \color{white} }
\DeclareCaptionFormat{listing}{
  \colorbox[cmyk]{0.43, 0.35, 0.35,0.01 }{
    \parbox{\textwidth}{\hspace{15pt}#1#2#3}
  }
}
\captionsetup[lstlisting]{ format=listing, labelfont=white, textfont=white, singlelinecheck=false, margin=0pt, font={bf,footnotesize} }

% Language Definitions for SPARQL
\lstdefinelanguage{sparql}{
morestring=[b][\color{blue}]\",
morekeywords={SELECT,CONSTRUCT,DESCRIBE,ASK,WHERE,FROM,NAMED,PREFIX,BASE,OPTIONAL,FILTER,GRAPH,LIMIT,OFFSET,SERVICE,UNION,EXISTS,NOT,BINDINGS,MINUS,a},
sensitive=true
}

\colorlet{punct}{red!60!black}
\definecolor{background}{HTML}{EEEEEE}
\definecolor{delim}{RGB}{20,105,176}
\colorlet{numb}{magenta!60!black}

\lstdefinelanguage{json}{
    basicstyle=\normalfont\ttfamily\footnotesize,
    numbersep=8pt,
    showstringspaces=false,
    breaklines=true,
    literate=
     *{0}{{{\color{numb}0}}}{1}
      {1}{{{\color{numb}1}}}{1}
      {2}{{{\color{numb}2}}}{1}
      {3}{{{\color{numb}3}}}{1}
      {4}{{{\color{numb}4}}}{1}
      {5}{{{\color{numb}5}}}{1}
      {6}{{{\color{numb}6}}}{1}
      {7}{{{\color{numb}7}}}{1}
      {8}{{{\color{numb}8}}}{1}
      {9}{{{\color{numb}9}}}{1}
      {:}{{{\color{punct}{:}}}}{1}
      {,}{{{\color{punct}{,}}}}{1}
      {\{}{{{\color{delim}{\{}}}}{1}
      {\}}{{{\color{delim}{\}}}}}{1}
      {[}{{{\color{delim}{[}}}}{1}
      {]}{{{\color{delim}{]}}}}{1},
}

\lstset{frame=single,captionpos=b}

\begin{document}

\maketitle
\tableofcontents

\chapter{Introduction}

Media companies produce ever larger numbers of articles, videos, podcasts,
games, etc. -- commonly collectively known as ``content''. A successful
content-producing website not only has to develop systems to aid producing and
publishing that content, but there are also demands to engineer effective
mechanisms to aid consumers in finding that content.

Approaches used in industry include providing a text-based search, hierarchical
categorisation (and thus navigation thereof) and even more tailored recommended
content based on past behaviour or content enjoyed by friends (or sometimes
simply other consumers who share your preferences).

\section{Problems}

There are several technical and conceptual problems with building effective
content discovery mechanisms, including:

\begin{itemize}

\item Large organisations can have content across multiple content management
systems, in differing formats and data models. Organisations face a large-scale
enterprise integration problem simply trying to gain a holistic view of all
their content.

\item Many content items are in fairly opaque formats, e.g. video content may be
stored as audio-visual binary data with minimal metadata to display on a
containing web page. Video content producers may not be motivated to provide
data attributes that might ultimately be most useful in determining if a user
will enjoy the video.

\item Content is being published continuously, which means any search or
discovery system needs to keep up with content as it is published and process it
into the appropriate data structures. Any machine learning previously performed
on the data set may need to be re-run.

\end{itemize}

\section{Hypothesis}

The following hypotheses are proposed for gaining new insights about an
organisation's diverse corpus of content:

\begin{itemize}

\item Research and software tools around the concept of \emph{Linked Data} can
aid us in rapidly acquiring a broad view (perhaps at the expense of depth) of an
organisation's content whilst also providing a platform for simple enrichment of
that content's metadata.

\item We can establish at least a na\"ive mapping of an RDF graph representing a
content item to an attribute set suitable for data mining. With such a mapping,
we can explore applying machine learning -- particularly unsupervised learning
-- across an organisation's whole content corpus.

\item Linked Data and Semantic Web \emph{ontologies} and models available can
provide data enrichment beyond attributes and keywords explicitly avaiable
within content data or metadata.

\item We can adapt established machine learning approaches such as clustering
for data published continuously in real time.

\item Many content-producers currently enrich their web pages with small
amounts of semantic metadata to provide better presentation of that content
as it is shared on social media. This enables simple collection of a full
breadth of content with significantly less effort than direct integration
with content management systems.

\end{itemize}

\chapter{Background}

This chapter discusses some of the existing research and technologies around
machine learning, RDF and combining them. It also covers some of the advantages
of using linked data and RDF in an enterprise setting and what tools and
approaches are well-defined enough that a corporation could build on top of
them rapidly.

Data mining activities such as machine learning rely on structuring data as
\emph{feature sets}\cite{bishop2006pattern} -- a set or vector of properties or
attributes that describe a single entity.
The process of \emph{feature extraction}
generates such feature sets from raw data and is a necessary early phase for
many machine learning activities.

The rest of this chapter will show:

\begin{enumerate}
\item that extracting feature sets from RDF\footnote{http://www.w3.org/TR/PR-rdf-syntax/} graphs can be done elegantly and follows naturally from some
previous work in this area; and
\item that the RDF graph is a suitable and even desirable data model for content
metadata in terms of acquiring, enriching and even transforming that data ahead
of feature extraction.
\end{enumerate}

\section{RDF and Feature Extraction}

The RDF graph is a powerful model
for metadata based on representing knowledge as a set of
subject-predicate-object \emph{triples}. The query language, SPARQL, gives us a
way to query the RDF graph structure using a declarative pattern and return a
set of all variable bindings that satisfy that pattern.

For example, the SPARQL query in Listings~\ref{lst:sparqlfoaf}
queries an RDF graph that contains contact information and returns the
names and email address of all ``Person'' entities therein.

Notably, Kiefer, Bernstein and Locher\cite{kiefer2008adding} proposed a novel
approach called SPARQL-ML -- an extension to the
SPARQL\cite{segaran2009programming} query language with new keywords to
facilitate both generating and applying models. This means that the system
capable of parsing and running standard queries must also run machine learning
algorithms.

Their work involved developing an extension to the SPARQL query
engine for \emph{Apache Jena}\footnote{https://jena.apache.org/} that integrates
with systems such as \emph{Weka}\footnote{http://www.cs.waikato.ac.nz/ml/weka/}.
A more suitable software application for enterprise use might focus solely on
converting RDF graphs into a neutral data structure that can plug into arbitrary
data mining algorithms.

\begin{lstlisting}[label=lst:sparqlfoaf,caption={Example SPARQL query for people's names and email addresses},language=sparql]
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT ?name ?email
WHERE {
  ?person a foaf:Person.
  ?person foaf:name ?name.
  ?person foaf:mbox ?email.
}
\end{lstlisting}

If we consider an RDF graph, $g$, to be expressed as a set of triples:

\begin{displaymath}
  (s, p, o) \in g
\end{displaymath}

\noindent this query could then be
expressed as function $f: G \rightarrow (S \times S)$ where $G$ is the set of
all possible RDF graphs and $S$ is a set of all possible strings.
This allows the result of the
SPARQL query to be expressed as a set of all SELECT variable bindings that
satisfy the WHERE clause:

$$
q(g,n,e) = \exists p . (p, type, Person) \in g\ \land (p, name, n) \in g \land (p, mbox, e) \in g
$$

$$
g \in G \models f(g) = \{(n, e) \subseteq (S \times S) \: | \: q(g,n,e)\}
$$

This could be generalised to express a given feature set as
vector $(a_1, a_2, ..., a_n)$:

$$
g \in G \models (a_1, a_2, ..., a_n) \in f(g)
$$

\noindent and in the case where all $a_k \in f(g)$ are literal (e.g. string or
numeric) values, we can thus consider a given SPARQL query to be specific
function capable of feature extraction from any RDF graph into sets of
categorical or numeric features.

\begin{lstlisting}[label=lst:sparqlabout,caption={SPARQL query to determine what },language=sparql]
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
SELECT ?topic
WHERE {
  ?article rdf:about ?topic .
}
\end{lstlisting}

This might allow a query that extracts a country's population, GDP, etc.
provide feature extraction for learning patterns in economics, for example.
However, this is limited to features derived from single-valued predicates
with literal-valued ranges. It is not clear how to formulate a query that
expresses whether or not a content item is about a given topic.

In the RDF
model, it would be more appropriate to use a query like that in
Listings~\ref{lst:sparqlabout} where for a given $?article$ identified by
URI, we can get a list of URIs identifying concepts which the article mentions.
Such a query might be expressed as function $f': G \rightarrow \mathcal P(U)$ where $U$ is
set of all URIs such that:

$$
g \in G \models f'(g, uri) = \{t \: | \: (uri, about, t) \in g\} 
$$

An approach of generating attributes for a given resource was proposed by
Paulheim and F\:urnkranz\cite{paulheim2012unsupervised}. They defined specific
SPARQL queries and provided case study evidence for the effectiveness of
each strategy.

Their work focused on starting with relational-style data (e.g. from a
relational database) and using \emph{Linked Open Data} to identify entities
within literal values in those relations and generated attributes from
SPARQL queries over those entities.

For a large content-producer, there is a more general problem where many content
items do not have a relational representation and the content source is a body
of text or even a raw HTML page. However, the feature generation from Paulheim
and F\:urnkranz proves to be a promising strategy given we can acquire an RDF
graph model for content items in the first place.

\section{RDF in the enterprise}
\label{sec:linked-enterprise-data}

\chapter{System Design}

In this chapter, a system is inductively derived and concretely design to make
use of multiple strategies for:

\begin{enumerate}
\item gathering (meta)data about all of an organisations content items;
\item extracting metadata not explicitly modelled in source content management
systems;
\item further enriching that metadata with information not explicitly present
in the content item itself; and
\item applying machine learning to that content metadata to gain new insights
about that content.
\end{enumerate}

Initially, a business context is described to produce a design for a system
that could be a applied within a media or content-producing organisation. This
context will guide all design decisions.

\section{Context}

\section{Use Cases}

\section{Data Pipeline}

A core subsystem in the overall system is a conceptual data pipeline whose
input is a URI or IRI identifying a content item published on an organisation's
website and the output is feature sets ready for applying machine learning.

In this section, a theoretical pipeline is inductively defined in steps such
that an application of this pipeline would choose to implement some subset of
all potential pipeline stages as appropriate for the relevant problem domain.

In Chapter~\ref{chp:implementation}, aa system is engineered that implements
as many of these pipeline stages as possible such that a running instance of
the application can configure which components to use and which not to use.
Then in Chapter~\ref{chp:evaluation}, an evalution of the system is given
while it is running each component in isolation to demonstrate which of the
theoretically-defined processes in this chapter appears to be most effective
in generating feature sets specifically for clustering web content.

This system requires some initial definition of some data structures in use:

\begin{description}

\item[IRI] \hfill \\
The input to the system is a character string conformant to the
IRI syntax defined in RFC 3987\footnote{http://tools.ietf.org/html/rfc3987}.
This allows more generality offered by
URIs\footnote{http://tools.ietf.org/html/rfc3986} but is trivially made
compatible with systems that use URIs through the conversion algorithm defined
in section~3.2 of RFC 3987. Note that the public URL by which the public can
read or otherwise consume the content is a valid identifier, but we are not
restricted to that.

\item[Feature Set] \hfill \\
The final output of this data pipeline is a data structure
analogous to a relation or tuple per IRI fed into the system. Every IRI should
have a literal value against all possible columns or fields. For binary fields,
(e.g. the presence of absence of a concept tag), a more pragmatic structure
might be a list of tags positively associated with the IRI rather than
explicitly assigning $false$ to all tags to which the content item does not
pertain. This is analogous to a spare matrix when dealing with a large number
of dimensions.

\item[Named RDF Graph] \hfill \\
The structure used throughout most of the data pipeline
is that of an RDF graph. This is used for all the benefits outlined in
Section~\ref{sec:linked-enterprise-data} such as ease of transformation and
combining of data sets. Named graphs are used such that all data acquired
are keyed back to the IRI of the content item being processed. This also allows
all graphs to be combined in a \emph{triplestore} if needed to allow SPARQL
queries across the combined data for all content items. This can be modelled as
a data structure in many programming languages, but where a serialisation is
used (e.g. examples shown here or to send the data between components), the
JSON-LD\cite{sporny2014json} syntax will be used.
\end{description}

\begin{centering}
\begin{lstlisting}[label=lst:jsonld-identity,caption={Identity graph for a content item in JSON-LD syntax},language=json]
{
  "@id": "http://example.com/entity/1",
  "@graph": []
}
\end{lstlisting}
\end{centering}

With the knowledge only of a content item's IRI, we are arguably only able to
produce an empty named RDF graph. Such a graph for an example IRI
\texttt{http://example.com/entity/} is illustrated in
JSON-LD syntax in Listings~\ref{lst:jsonld-identity}.

The most na\"ive feature set we can generate from such an RDF graph is clearly
a singleton relation \texttt{("http://example.com/entity/")} where a single
$IRI$ field has the value \texttt{"http://example.com/entity/"}. It is also
clear that a set of one-dimension feature vectors with unique values in each
is not suitable for any form of machine learning activity. This does, however,
illustrate a baseline for a working software application that is -- at least
in the syntactic sense -- transforming IRI inputs to feature sets outputs.
Such a $null$ feature generator is depicted in Figure~\ref{fig:gen-null}.

\begin{figure}[h]
  \begin{center}
    \begin{dot2tex}[dot,options=-t math,autosize,pgf]
      digraph g {
        rankdir=LR;

        node [shape=circle,margin="0,0"];
        edge [len=2];

        IRI -> RDF [label="extract"];
        RDF -> Features [label="generate"];
      }
    \end{dot2tex}
  \end{center}
  \caption{Null feature generator \label{fig:gen-null}}
\end{figure}

Note that Figure~\ref{fig:gen-null} shows all three data structures involved
despite having no functional use. We can also see top-level definitions of the
process where we first \emph{extract} semantic information in RDF from a
content item indentified by IRI and then \emph{generate} features therefrom.
More useful models can now be inductively defined by adding atomic
subcomponents that may each add value to the overall transformation.

There are two clear axes along which we can improve this pipeline: expand
the size of the RDF graph we are extracting and improve how we convert this
graph into features. In the first instance, we can consider the former and add
a single pipeline stage for expanding the RDF graph.

Tim Berners-Lee outlined four rules\cite{berners2011linked} for Linked Data,
rule number three of which states ``When someone looks up a URI, provide useful
information, using the standards''. If we assume that many pages have embedded
some semantic web or RDF data, then a simple extraction strategy would be
to deference the content item's IRI via an HTTP GET and pass the content
to a parser capable of extracting RDFa, microformats, etc.

\begin{figure}[h]
  \begin{center}
    \begin{dot2tex}[dot,options=-t math,autosize,pgf]
      digraph g {
        rankdir=LR;

        node [shape=circle,margin="0,0"]

        IRI -> RDFa [label="fetch"];
        RDFa -> RDF [label="parse"];
        RDF -> Features [label="generate"];
      }
    \end{dot2tex}
  \end{center}
  \caption{Semantic web content miner}
\end{figure}

\begin{sidewaysfigure}[h]
  \begin{center}
    \begin{dot2tex}[dot,options=-t math,autosize,pgf]
      digraph g {
        rankdir=LR;

        node [shape=circle,margin="0,0"];

        dummy [shape=none,label=""];
        
        IRI [label="IRI"];
        RDF1 [label="RDF_1"];
        RDF2 [label="RDF_2"];
        RDFp [label="RDF'"];
        
        IRI -> RDFa [label="fetch"];
        RDFa -> RDF [label="parse"];
        RDF -> RDF1 [label="DBPedia"];
        RDF -> RDF2 [label="DBPedia"];
        RDF -> RDFp [label="\cup"];
        RDF1 -> RDFp [label="\cup"];
        RDF2 -> RDFp [label="\cup"];
        RDFp -> Features [label="generate"];
      }
    \end{dot2tex}
  \end{center}
  \caption{Semantic web content miner with DBPedia enrichment}
\end{sidewaysfigure}

\begin{figure}[h]
  \begin{center}
    \begin{dot2tex}[dot,options=-t math,autosize,pgf]
      digraph g {
        rankdir=LR;

        node [shape=circle,margin="0,0"]

        IRI -> RDFa [label="parse"];
        IRI -> Entities [label="extract"];
        IRI -> Linked [label="scrape"];


        RDFa -> RDF [label="\cup"];
        Entities -> RDF [label="\cup"];
        Linked -> RDF [label="\cup"];

        RDF -> Features [label="generate"];
      }
    \end{dot2tex}
  \end{center}
  \caption{Maximal web content miner}
\end{figure}

\section{Technical Architecture}


\chapter{Implementation}
\label{chp:implementation}

\chapter{Evaluation}
\label{chp:evaluation}

\chapter{Conclusion}

\bibliographystyle{plain}
\bibliography{references}

\end{document}
