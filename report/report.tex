\documentclass[10pt,a4paper]{report}

\title{Improving content discovery through combining linked data and data mining techniques}

\author{Ross Fenning}

\usepackage{nag}
\usepackage{rotating}
\usepackage[autosize]{dot2texi}
\usepackage[Bjarne]{fncychap}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,decorations}
\usepackage{listings}
\usepackage{textcomp}
\usepackage{mathtools}
\usepackage{caption}

\DeclareCaptionFont{white}{ \color{white} }
\DeclareCaptionFormat{listing}{
  \colorbox[cmyk]{0.43, 0.35, 0.35,0.01 }{
    \parbox{\textwidth}{\hspace{15pt}#1#2#3}
  }
}
\captionsetup[lstlisting]{ format=listing, labelfont=white, textfont=white, singlelinecheck=false, margin=0pt, font={bf,footnotesize} }


\begin{document}

\maketitle
\tableofcontents

\chapter{Introduction}

Media companies produce ever larger numbers of articles, videos, podcasts,
games, etc. -- commonly collectively known as ``content''. A successful
content-producing website not only has to develop systems to aid producing and
publishing that content, but there are also demands to engineer effective
mechanisms to aid consumers in finding that content.

Approaches used in industry include providing a text-based search, hierarchical
categorisation (and thus navigation thereof) and even more tailored recommended
content based on past behaviour or content enjoyed by friends (or sometimes
simply other consumers who share your preferences).

\section{Problems}

There are several technical and conceptual problems with building effective
content discovery mechanisms, including:

\begin{itemize}

\item Large organisations can have content across multiple content management
systems, in differing formats and data models. Organisations face a large-scale
enterprise integration problem simply trying to gain a holistic view of all
their content.

\item Many content items are in fairly opaque formats, e.g. video content may be
stored as audio-visual binary data with minimal metadata to display on a
containing web page. Video content producers may not be motivated to provide
data attributes that might ultimately be most useful in determining if a user
will enjoy the video.

\item Content is being published continuously, which means any search or
discovery system needs to keep up with content as it is published and process it
into the appropriate data structures. Any machine learning previously performed
on the data set may need to be re-run.

\end{itemize}

\section{Hypothesis}

The following hypotheses are proposed for gaining new insights about an
organisation's diverse corpus of content:

\begin{itemize}

\item Research and software tools around the concept of \emph{Linked Data} can
aid us in rapidly acquiring a broad view (perhaps at the expense of depth) of an
organisation's content whilst also providing a platform for simple enrichment of
that content's metadata.

\item We can establish at least a na\"ive mapping of an RDF graph representing a
content item to an attribute set suitable for data mining. With such a mapping,
we can explore applying machine learning -- particularly unsupervised learning
-- across an organisation's whole content corpus.

\item Linked Data and Semantic Web \emph{ontologies} and models available can
provide data enrichment beyond attributes and keywords explicitly avaiable
within content data or metadata.

\item We can adapt established machine learning approaches such as clustering
for data published continuously in real time.

\item Many content-producers currently enrich their web pages with small
amounts of semantic metadata to provide better presentation of that content
as it is shared on social media. This enables simple collection of a full
breadth of content with significantly less effort than direct integration
with content management systems.

\end{itemize}

\chapter{Background}

Data mining activities such as machine learning rely on structuring data as
\emph{feature sets}\cite{bishop2006pattern} -- a set or vector of properties or
attributes that describe a single entity. The process of \emph{feature extraction}
generates such feature sets from raw data and is a necessary early phase for
many machine learning activities.

The RDF\footnote{http://www.w3.org/TR/PR-rdf-syntax/} graph is a powerful model
for metadata based on representing knowledge as a set of subject-predicate-object
\emph{triples}. The query language, SPARQL, gives us a way to query the RDF
graph structure using a declarative pattern and return a set of all variable
bindings that satisfy that pattern.

\begin{lstlisting}[label=lst:sparqlfoaf,caption={Example SPARQL query for people's names and email addresses},frame=single,captionpos=b]
  PREFIX foaf: <http://xmlns.com/foaf/0.1/>
  SELECT ?name ?email
  WHERE {
    ?person a foaf:Person.
    ?person foaf:name ?name.
    ?person foaf:mbox ?email.
  }
\end{lstlisting}

For example, the following SPARQL in Listings~\ref{sparqlfoaf}
queries an RDF graph that contains contact information and returns the
names and email address of all ``Person'' entities therein. This query could be
expressed as function $f: G \rightarrow S$ from the set of all graphs, $G$, to
set, $S$, of values requested in the SELECT statement such that:

\begin{displaymath}
  S = \{(n, e) | \exists p . (p, type, Person) \in G\ \land (p, name, n) \in G \land (p, mbox, e) \in G\}
\end{displaymath}

This could be generalised:

\begin{displaymath}
  S = \{(a_1, a_2, ..., a_n) | f(G) \}
\end{displaymath}

\noindent and in the case where all $a_k \in S$ are literal (e.g. string or
numeric) values, we can thus consider a given SPARQL query to be specific function
capable of feature extraction from any RDF graph into sets of categorical or
numeric features.

Notably, Kiefer, Bernstein and Locher\cite{kiefer2008adding} proposed a novel
approach called SPARQL-ML -- an extension to the
SPARQL\cite{segaran2009programming} query language with new keywords to
facilitate both generating and applying models. This means that the system capable
of parsing and running standard queries must also run machine learning
algorithms. Their work involved developing an extension to the SPARQL query
engine for \emph{Apache Jena}\footnote{https://jena.apache.org/} that integrates
with systems such as \emph{Weka}\footnote{http://www.cs.waikato.ac.nz/ml/weka/}.
A more suitable software application for enterprise use might focus solely on
converting RDF graphs into a neutral data structure that can plug into arbitrary
data mining algorithms.

\chapter{System Design}

\begin{figure}[h]
  \begin{center}
    \begin{dot2tex}[dot,options=-t math,autosize,pgf]
      digraph g {
        rankdir=LR;

        node [shape=circle,margin="0,0"];
        edge [len=2];

        URI -> RDF [label="extract"];
        RDF -> Features [label="generate"];
      }
    \end{dot2tex}
  \end{center}
  \caption{Null content miner}
\end{figure}

\begin{figure}[h]
  \begin{center}
    \begin{dot2tex}[dot,options=-t math,autosize,pgf]
      digraph g {
        rankdir=LR;

        node [shape=circle,margin="0,0"]

        URI -> RDFa [label="fetch"];
        RDFa -> RDF [label="parse"];
        RDF -> Features [label="generate"];
      }
    \end{dot2tex}
  \end{center}
  \caption{Semantic web content miner}
\end{figure}

\begin{sidewaysfigure}[h]
  \begin{center}
    \begin{dot2tex}[dot,options=-t math,autosize,pgf]
      digraph g {
        rankdir=LR;

        node [shape=circle,margin="0,0"];

        dummy [shape=none,label=""];
        
        URI [label="URI"];
        RDF1 [label="RDF_1"];
        RDF2 [label="RDF_2"];
        RDFp [label="RDF'"];
        
        URI -> RDFa [label="fetch"];
        RDFa -> RDF [label="parse"];
        RDF -> RDF1 [label="DBPedia"];
        RDF -> RDF2 [label="DBPedia"];
        RDF -> RDFp [label="\cup"];
        RDF1 -> RDFp [label="\cup"];
        RDF2 -> RDFp [label="\cup"];
        RDFp -> Features [label="generate"];
      }
    \end{dot2tex}
  \end{center}
  \caption{Semantic web content miner with DBPedia enrichment}
\end{sidewaysfigure}

\begin{figure}[h]
  \begin{center}
    \begin{dot2tex}[dot,options=-t math,autosize,pgf]
      digraph g {
        rankdir=LR;

        node [shape=circle,margin="0,0"]

        URI -> RDFa [label="parse"];
        URI -> Entities [label="extract"];
        URI -> Linked [label="scrape"];


        RDFa -> RDF [label="\cup"];
        Entities -> RDF [label="\cup"];
        Linked -> RDF [label="\cup"];

        RDF -> Features [label="generate"];
      }
    \end{dot2tex}
  \end{center}
  \caption{Maximal web content miner}
\end{figure}


\chapter{Implementation}
\chapter{Evaluation}
\chapter{Conclusion}

\bibliographystyle{plain}
\bibliography{references}

\end{document}
